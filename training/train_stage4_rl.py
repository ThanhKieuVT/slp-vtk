#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# KH√îNG C·∫¶N THI·∫æT L√öC N√ÄY

"""
STAGE 4: REINFORCEMENT LEARNING FINE-TUNING
K·ªπ thu·∫≠t: Reward-Weighted Regression (RWR) - ·ªîn ƒë·ªãnh h∆°n PPO cho Flow Matching.
"""
import os
import sys
import argparse
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm

sys.path.append(os.getcwd())
from dataset import SignLanguageDataset, collate_fn
from models.fml.latent_flow_matcher import LatentFlowMatcher
# Gi·∫£ s·ª≠ ch·ªã c√≥ module ƒë√°nh gi√° (n·∫øu ch∆∞a c√≥ th√¨ d√πng h√†m dummy b√™n d∆∞·ªõi)
# from metrics import compute_back_translation_score 

def dummy_reward_function(poses, text_tokens):
    """
    H√†m gi·∫£ l·∫≠p ph·∫ßn th∆∞·ªüng (Reward).
    Trong th·ª±c t·∫ø, ch·ªã thay th·∫ø b·∫±ng:
    1. Back-Translation Model (D·ªãch pose ng∆∞·ª£c l·∫°i text -> so s√°nh ƒë·ªô ƒë√∫ng)
    2. Smoothness (1 / ƒë·ªô rung l·∫Øc c·ªßa joint)
    """
    # V√≠ d·ª•: Th∆∞·ªüng cho chuy·ªÉn ƒë·ªông m∆∞·ª£t (v·∫≠n t·ªëc nh·ªè)
    # poses: [B, T, D]
    velocity = poses[:, 1:] - poses[:, :-1]
    smoothness_reward = -torch.mean(velocity.abs(), dim=(1,2)) # C√†ng √≠t rung c√†ng t·ªët
    
    # ·ªû ƒë√¢y em gi·∫£ l·∫≠p ƒëi·ªÉm ng·∫´u nhi√™n ƒë·ªÉ code ch·∫°y ƒë∆∞·ª£c
    # Ch·ªã h√£y PLUG MODEL SIGN RECOGNITION C·ª¶A CH·ªä V√ÄO ƒê√ÇY
    fake_semantic_score = torch.rand(poses.shape[0], device=poses.device)
    
    total_reward = fake_semantic_score + (0.1 * smoothness_reward)
    return total_reward

def main():
    p = argparse.ArgumentParser()
    p.add_argument("--data_dir", type=str, required=True)
    p.add_argument("--student_ckpt", type=str, required=True, help="Checkpoint t·ª´ Stage 3")
    p.add_argument("--save_dir", type=str, default="./ckpts_stage4_rl")
    p.add_argument("--lr", type=float, default=1e-5) # LR c·ª±c nh·ªè cho RL
    p.add_argument("--samples_per_prompt", type=int, default=4, help="Sinh bao nhi√™u m·∫´u ƒë·ªÉ ch·ªçn l·ªçc")
    args = p.parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    os.makedirs(args.save_dir, exist_ok=True)

    # Load Data & Model
    train_loader = DataLoader(
        SignLanguageDataset(args.data_dir, split="train", max_seq_len=80), # Sequence ng·∫Øn h∆°n cho d·ªÖ h·ªçc
        batch_size=8, shuffle=True, collate_fn=collate_fn
    )
    
    student = LatentFlowMatcher(latent_dim=256, hidden_dim=384).to(device)
    ckpt = torch.load(args.student_ckpt, map_location=device)
    student.load_state_dict(ckpt['model_state_dict'], strict=False)
    student.train()
    
    latent_scale = float(ckpt.get("latent_scale_factor", 1.0))
    optimizer = torch.optim.AdamW(student.parameters(), lr=args.lr)

    print("üöÄ START STAGE 4: RL FINE-TUNING (Reward Weighted)")
    
    for epoch in range(10): # RL ch·ªâ c·∫ßn v√†i epoch
        total_reward_avg = 0
        pbar = tqdm(train_loader, desc=f"Ep {epoch+1}")
        
        for batch in pbar:
            # 1. Chu·∫©n b·ªã d·ªØ li·ªáu
            text_tokens = batch[1].to(device)
            attention_mask = batch[2].to(device)
            batch_dict = {'text_tokens': text_tokens, 'attention_mask': attention_mask}
            
            # 2. Sinh m·∫´u (Sampling)
            # Student sinh ra K m·∫´u cho c√πng 1 c√¢u l·ªánh
            # L∆∞u √Ω: Flow Matcher c·∫ßn ch·∫ø ƒë·ªô inference ƒë·ªÉ sinh
            student.eval() 
            generated_trajs = []
            
            with torch.no_grad():
                # L·∫∑p l·∫°i batch K l·∫ßn ƒë·ªÉ sinh nhi·ªÅu bi·∫øn th·ªÉ
                expanded_batch_dict = {
                    k: v.repeat_interleave(args.samples_per_prompt, dim=0) 
                    for k, v in batch_dict.items()
                }
                
                # G·ªçi h√†m sample (sinh t·ª´ noise -> latent)
                # Ch·ªã c·∫ßn ƒë·∫£m b·∫£o class LatentFlowMatcher c√≥ h√†m .sample()
                # N·∫øu ch∆∞a c√≥, n√≥ l√† qu√° tr√¨nh gi·∫£i ODE (Euler step)
                latents_pred = student.sample(
                    batch=expanded_batch_dict, 
                    steps=10, # √çt step cho nhanh
                    device=device
                )
                
            # 3. T√≠nh Reward
            rewards = dummy_reward_function(latents_pred, expanded_batch_dict['text_tokens'])
            
            # Chu·∫©n h√≥a Reward trong batch (ƒë·ªÉ ·ªïn ƒë·ªãnh)
            rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)
            weights = torch.exp(rewards) # Bi·∫øn Reward th√†nh tr·ªçng s·ªë (Weight)

            # 4. Update Student (Maximizing Reward)
            # Ch√∫ng ta train Student sao cho n√≥ sinh ra output GI·ªêNG v·ªõi c√°c m·∫´u c√≥ Reward cao
            student.train()
            optimizer.zero_grad()
            
            # Forward pass l·∫°i v·ªõi c√°c m·∫´u ƒë√£ sinh (nh∆∞ng gi·ªù c√≥ Gradient)
            # M·ª•c ti√™u: T·ªëi ∆∞u h√≥a x√°c su·∫•t sinh ra c√°c m·∫´u t·ªët (Weighted Regression)
            losses = student(
                batch=expanded_batch_dict,
                gt_latent=latents_pred.detach(), # Coi m·∫´u v·ª´a sinh l√† Target
                pose_gt=None,
                mode="train"
            )
            
            # Loss ƒë∆∞·ª£c nh√¢n v·ªõi Weight (Reward)
            # M·∫´u n√†o Reward th·∫•p -> Weight th·∫•p -> √çt ·∫£nh h∆∞·ªüng Gradient
            weighted_loss = (losses['total'] * weights).mean()
            
            weighted_loss.backward()
            torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)
            optimizer.step()
            
            total_reward_avg += rewards.mean().item()
            pbar.set_postfix({'Rw': f"{total_reward_avg / (pbar.n + 1):.4f}"})

        # Save
        torch.save(student.state_dict(), os.path.join(args.save_dir, "best_student_rl.pt"))

if __name__ == "__main__":
    main()