"""
Inference Script: Text ‚Üí Latent Flow ‚Üí Pose
FIXED: B·ªï sung logic t·∫£i Normalization Stats v√† Denormalize Pose.
"""
import os
import argparse
import torch
import numpy as np
from transformers import BertTokenizer
import time

# --- IMPORT MODEL V√Ä H√ÄM DENORMALIZE ---
try:
    from models.fml.autoencoder import UnifiedPoseAutoencoder
    from models.fml.latent_flow_matcher import LatentFlowMatcher
    # ‚úÖ FIX: C·∫ßn h√†m denormalize_pose v√† h√†m n√†y n·∫±m trong data_preparation
    from data_preparation import denormalize_pose 
except ImportError as e:
    print(f"‚ùå L·ªói Import: {e}. H√£y ƒë·∫£m b·∫£o c√°c module v√† h√†m denormalize_pose ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a.")
    import sys
    sys.exit(1)

def inference_sota(
    text,
    flow_matcher,
    decoder,
    tokenizer,
    device,
    scale_factor=1.0, 
    num_steps=50,
    normalize_stats=None # ‚úÖ FIX: Nh·∫≠n normalization_stats
):
    flow_matcher.eval()
    decoder.eval()
    
    start_time = time.time()
    
    # 1. Tokenize text
    encoded = tokenizer(text, return_tensors='pt', padding=True).to(device)
    text_tokens = encoded['input_ids']
    attention_mask = encoded['attention_mask']
    
    # 2. Encode Text Features 
    with torch.no_grad():
        text_features, text_mask = flow_matcher.encode_text(text_tokens, attention_mask)
    
    # 3. Flow Matching Inference
    print(f"üîÑ ƒêang sinh Latent (Steps={num_steps})...")
    
    generated_latent = flow_matcher._inference_forward(
        batch={}, 
        text_features=text_features, 
        text_mask=text_mask, 
        num_steps=num_steps
    ) # [1, T, 256]
    
    # 4. UN-SCALE LATENT (QUAN TR·ªåNG NH·∫§T)
    generated_latent = generated_latent / scale_factor
    
    # 5. Decode ra Pose
    with torch.no_grad():
        pose_norm = decoder(generated_latent) # [1, T, 214] (Pose ƒê√É CHU·∫®N H√ìA)
        pose = pose_norm.squeeze(0).cpu().numpy()  # [T, 214] (numpy)

    # 6. Post-process: DENORMALIZE B·∫ÆT BU·ªòC
    if normalize_stats is not None:
        mean = normalize_stats['mean']
        std = normalize_stats['std']
        # ‚úÖ FIX: √Åp d·ª•ng Denormalization ƒë·ªÉ pose c√≥ t·ªça ƒë·ªô v·∫≠t l√Ω ƒë√∫ng
        pose = denormalize_pose(pose, mean, std) 
        print("‚úÖ Pose Denormalized.")

    latency = time.time() - start_time
    
    return pose, latency

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--text', type=str, required=True, help='C√¢u text ƒë·∫ßu v√†o')
    parser.add_argument('--flow_checkpoint', type=str, required=True, help='Checkpoint Flow (Stage 2)')
    parser.add_argument('--autoencoder_checkpoint', type=str, required=True, help='Checkpoint AE (Stage 1)')
    parser.add_argument('--output_path', type=str, default='output_pose.npy', help='N∆°i l∆∞u file .npy')
    parser.add_argument('--data_dir', type=str, required=True, help='Th∆∞ m·ª•c ch·ª©a normalization_stats.npz') # ‚úÖ FIX: data_dir B·∫ÆT BU·ªòC
    
    # C√°c tham s·ªë Model (Ph·∫£i kh·ªõp l√∫c train)
    parser.add_argument('--latent_dim', type=int, default=256)
    parser.add_argument('--hidden_dim', type=int, default=512)
    parser.add_argument('--num_steps', type=int, default=50, help='S·ªë b∆∞·ªõc l·∫•y m·∫´u (10-50)')
    
    # C√°c c·ªù t√≠nh nƒÉng (Ph·∫£i kh·ªõp l√∫c train)
    parser.add_argument('--use_ssm_prior', action='store_true')
    parser.add_argument('--use_sync_guidance', action='store_true')
    
    args = parser.parse_args()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üöÄ Using device: {device}")
    
    # --- LOAD NORMALIZATION STATS (FIX) ---
    normalize_stats = None
    stats_path = os.path.join(args.data_dir, "normalization_stats.npz")
    if os.path.exists(stats_path):
        normalize_stats = np.load(stats_path)
        print(f"‚úÖ Loaded normalization stats from {stats_path}")
    else:
        print(f"‚ùå ERROR: normalization_stats.npz NOT found at {stats_path}. Cannot Denormalize!")
        
    
    # 1. Load Autoencoder
    print(f"\nüì¶ Loading Autoencoder...")
    ae = UnifiedPoseAutoencoder(pose_dim=214, latent_dim=args.latent_dim, hidden_dim=args.hidden_dim).to(device)
    ae.load_state_dict(torch.load(args.autoencoder_checkpoint, map_location=device)['model_state_dict'])
    ae.eval()
    
    # 2. Load Flow Matcher & Scale Factor
    print(f"üì¶ Loading Flow Matcher...")
    ckpt = torch.load(args.flow_checkpoint, map_location=device)
    
    scale_factor = ckpt.get('latent_scale_factor', 1.0)
    print(f"‚úÖ T√¨m th·∫•y Scale Factor: {scale_factor:.4f}")
    
    flow_matcher = LatentFlowMatcher(
        latent_dim=args.latent_dim, hidden_dim=args.hidden_dim,
        num_flow_layers=6, num_prior_layers=4, num_heads=8, dropout=0.1,
        use_ssm_prior=args.use_ssm_prior, use_sync_guidance=args.use_sync_guidance
    ).to(device)
    
    # Load weights (strict=False ƒë·ªÉ an to√†n n·∫øu thi·∫øu key linh tinh)
    try:
        flow_matcher.load_state_dict(ckpt['model_state_dict'], strict=False)
    except Exception as e:
        print(f"‚ö†Ô∏è Warning load weights: {e}")
    
    flow_matcher.eval()
    
    # 3. Tokenizer
    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
    
    # 4. Run Inference
    print(f"\nüé¨ Input: '{args.text}'")
    
    pose, latency = inference_sota(
        text=args.text,
        flow_matcher=flow_matcher,
        decoder=ae.decoder,
        tokenizer=tokenizer,
        device=device,
        scale_factor=scale_factor, 
        num_steps=args.num_steps,
        normalize_stats=normalize_stats # ‚úÖ FIX: Truy·ªÅn stats v√†o h√†m inference
    )
    
    print(f"‚úÖ Done! Shape: {pose.shape}")
    print(f"‚è±Ô∏è Latency: {latency:.2f}s")
    print(f"üíæ Saving to {args.output_path}")
    np.save(args.output_path, pose)

    # 5. G·ª£i √Ω visualize
    print(f"\nüí° Ti·∫øp theo: Ch·ªã h√£y ch·∫°y l·ªánh sau ƒë·ªÉ xem video:")
    print(f"python visualize_single_pose.py --npy_path {args.output_path} --output_video result.mp4")

if __name__ == '__main__':
    main()