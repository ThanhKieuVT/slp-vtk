"""
Inference Script: Text â†’ Latent Flow â†’ Pose
UPDATED: Há»— trá»£ Length Predictor tá»± Ä‘á»™ng & Scale Factor Correction
"""
import os
import argparse
import torch
import numpy as np
from transformers import BertTokenizer
import time

# --- IMPORT MODEL ---
try:
    from models.fml.autoencoder import UnifiedPoseAutoencoder
    from models.fml.latent_flow_matcher import LatentFlowMatcher
    # from data_preparation import denormalize_pose # Bá» qua náº¿u chá»‹ lÆ°u trá»±c tiáº¿p 214 Ä‘iá»ƒm
except ImportError as e:
    print(f"âŒ Lá»—i Import: {e}. HÃ£y cháº¡y script tá»« thÆ° má»¥c gá»‘c dá»± Ã¡n.")
    import sys
    sys.exit(1)

def inference_sota(
    text,
    flow_matcher,
    decoder,
    tokenizer,
    device,
    scale_factor=1.0, # Má»šI: Cáº§n tham sá»‘ nÃ y Ä‘á»ƒ pose khÃ´ng bá»‹ bÃ© tÃ­
    num_steps=50,
    manual_length=None # Náº¿u muá»‘n Ã©p Ä‘á»™ dÃ i (optional)
):
    flow_matcher.eval()
    decoder.eval()
    
    start_time = time.time()
    
    # 1. Tokenize text
    encoded = tokenizer(text, return_tensors='pt', padding=True).to(device)
    text_tokens = encoded['input_ids']
    attention_mask = encoded['attention_mask']
    
    # 2. Encode Text Features (LÃ m bÃªn ngoÃ i Ä‘á»ƒ clear logic)
    with torch.no_grad():
        text_features, text_mask = flow_matcher.encode_text(text_tokens, attention_mask)
    
    # 3. Flow Matching Inference
    # Model sáº½ Tá»° Äá»˜NG dá»± Ä‘oÃ¡n Ä‘á»™ dÃ i bÃªn trong hÃ m nÃ y
    # batch={} vÃ¬ ta khÃ´ng cÃ²n cáº§n target_length tá»« ngoÃ i ná»¯a (trá»« khi manual)
    
    # *Máº¹o*: Náº¿u chá»‹ muá»‘n Ã©p Ä‘á»™ dÃ i thá»§ cÃ´ng Ä‘á»ƒ test, chá»‹ cÃ³ thá»ƒ hack vÃ o hÃ m _inference_forward
    # nhÆ°ng máº·c Ä‘á»‹nh hÃ£y Ä‘á»ƒ model tá»± lÃ m.
    
    print(f"ğŸ”„ Äang sinh Latent (Steps={num_steps})...")
    # KHÃ”NG bá»c flow_matcher trong no_grad náº¿u dÃ¹ng Guidance (nhÆ°ng á»Ÿ Ä‘Ã¢y infer thuáº§n nÃªn ok)
    # Tuy nhiÃªn, hÃ m _inference_forward cá»§a chá»‹ Ä‘Ã£ cÃ³ torch.set_grad_enabled bÃªn trong logic guidance rá»“i
    # NÃªn gá»i bÃ¬nh thÆ°á»ng.
    
    generated_latent = flow_matcher._inference_forward(
        batch={}, 
        text_features=text_features, 
        text_mask=text_mask, 
        num_steps=num_steps
    ) # [1, T, 256]
    
    # 4. UN-SCALE LATENT (QUAN TRá»ŒNG NHáº¤T)
    # LÃºc train ta nhÃ¢n scale_factor, giá» pháº£i chia Ä‘i
    generated_latent = generated_latent / scale_factor
    
    # 5. Decode ra Pose
    with torch.no_grad():
        pose = decoder(generated_latent) # [1, T, 214]

    # 6. Post-process
    pose = pose.squeeze(0).cpu().numpy()  # [T, 214]
    latency = time.time() - start_time
    
    return pose, latency

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--text', type=str, required=True, help='CÃ¢u text Ä‘áº§u vÃ o')
    parser.add_argument('--flow_checkpoint', type=str, required=True, help='Checkpoint Flow (Stage 2)')
    parser.add_argument('--autoencoder_checkpoint', type=str, required=True, help='Checkpoint AE (Stage 1)')
    parser.add_argument('--output_path', type=str, default='output_pose.npy', help='NÆ¡i lÆ°u file .npy')
    
    # CÃ¡c tham sá»‘ Model (Pháº£i khá»›p lÃºc train)
    parser.add_argument('--latent_dim', type=int, default=256)
    parser.add_argument('--hidden_dim', type=int, default=512)
    parser.add_argument('--num_steps', type=int, default=50, help='Sá»‘ bÆ°á»›c láº¥y máº«u (10-50)')
    
    # CÃ¡c cá» tÃ­nh nÄƒng (Pháº£i khá»›p lÃºc train)
    parser.add_argument('--use_ssm_prior', action='store_true')
    parser.add_argument('--use_sync_guidance', action='store_true')
    
    args = parser.parse_args()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ Using device: {device}")
    
    # 1. Load Autoencoder
    print(f"ğŸ“¦ Loading Autoencoder...")
    ae = UnifiedPoseAutoencoder(pose_dim=214, latent_dim=args.latent_dim, hidden_dim=args.hidden_dim).to(device)
    ae.load_state_dict(torch.load(args.autoencoder_checkpoint, map_location=device)['model_state_dict'])
    ae.eval()
    
    # 2. Load Flow Matcher & Scale Factor
    print(f"ğŸ“¦ Loading Flow Matcher...")
    ckpt = torch.load(args.flow_checkpoint, map_location=device)
    
    # Láº¤Y SCALE FACTOR Tá»ª CHECKPOINT
    scale_factor = ckpt.get('latent_scale_factor', 1.0)
    print(f"âœ… TÃ¬m tháº¥y Scale Factor: {scale_factor:.4f}")
    
    flow_matcher = LatentFlowMatcher(
        latent_dim=args.latent_dim, hidden_dim=args.hidden_dim,
        num_flow_layers=6, num_prior_layers=4, num_heads=8, dropout=0.1,
        use_ssm_prior=args.use_ssm_prior, use_sync_guidance=args.use_sync_guidance
        # CÃ¡c tham sá»‘ loss weights khÃ´ng quan trá»ng lÃºc inference
    ).to(device)
    
    # Load weights (strict=False Ä‘á»ƒ an toÃ n náº¿u thiáº¿u key linh tinh)
    try:
        flow_matcher.load_state_dict(ckpt['model_state_dict'], strict=False)
    except Exception as e:
        print(f"âš ï¸ Warning load weights: {e}")
    
    flow_matcher.eval()
    
    # 3. Tokenizer
    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
    
    # 4. Run Inference
    print(f"\nğŸ¬ Input: '{args.text}'")
    
    pose, latency = inference_sota(
        text=args.text,
        flow_matcher=flow_matcher,
        decoder=ae.decoder,
        tokenizer=tokenizer,
        device=device,
        scale_factor=scale_factor, # Truyá»n scale factor vÃ o
        num_steps=args.num_steps
    )
    
    print(f"âœ… Done! Shape: {pose.shape}")
    print(f"â±ï¸ Latency: {latency:.2f}s")
    print(f"ğŸ’¾ Saving to {args.output_path}")
    np.save(args.output_path, pose)

    # 5. Gá»£i Ã½ visualize
    print(f"\nğŸ’¡ Tiáº¿p theo: Chá»‹ hÃ£y cháº¡y lá»‡nh sau Ä‘á»ƒ xem video:")
    print(f"python visualize_single_pose.py --npy_path {args.output_path} --output_video result.mp4")

if __name__ == '__main__':
    main()