code nÃ y thÃ¬ sao import os
import sys
import argparse
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
from torch.optim.lr_scheduler import CosineAnnealingWarmRestartsÂ 

sys.path.append(os.getcwd())Â 

try:
Â  Â  from dataset import SignLanguageDataset, collate_fn
Â  Â  from models.fml.autoencoder import UnifiedPoseAutoencoder
Â  Â  from models.fml.latent_flow_matcher import LatentFlowMatcherÂ 
except ImportError as e:
Â  Â  print(f"âŒ Lá»—i Import: {e}")
Â  Â  sys.exit(1)

# === 1. HÃ€M TÃNH SCALE FACTOR ===
def estimate_scale_factor(encoder, dataloader, device, num_batches=20):
Â  Â  print(f"â³ Äang tÃ­nh toÃ¡n Latent Scale Factor ({num_batches} batches)...")
Â  Â  encoder.eval()
Â  Â  all_latents = []
Â  Â Â 
Â  Â  with torch.no_grad():
Â  Â  Â  Â  for i, batch in enumerate(dataloader):
Â  Â  Â  Â  Â  Â  if i >= num_batches: break
Â  Â  Â  Â  Â  Â  poses = batch['poses'].to(device)
Â  Â  Â  Â  Â  Â  pose_mask = batch['pose_mask'].to(device)
Â  Â  Â  Â  Â  Â  z = encoder(poses, mask=pose_mask)
Â  Â  Â  Â  Â  Â  all_latents.append(z.cpu())
Â  Â Â 
Â  Â  all_latents = torch.cat(all_latents, dim=0)
Â  Â  std = all_latents.std()
Â  Â  scale_factor = 1.0 / (std.item() + 1e-6)
Â  Â Â 
Â  Â  print(f"âœ… Latent Std gá»‘c: {std.item():.4f}")
Â  Â  print(f"âœ… Scale Factor tá»± Ä‘á»™ng tÃ­nh: {scale_factor:.6f}")
Â  Â  return scale_factor

# === 2. HÃ€M TRAIN ===
def train_epoch(flow_matcher, encoder, dataloader, optimizer, scheduler, device, epoch, scale_factor, log_attn_freq=100):
Â  Â  flow_matcher.train()
Â  Â  encoder.eval()
Â  Â Â 
Â  Â  total_loss = 0.0
Â  Â  losses_log = {'flow': 0.0, 'sync': 0.0}
Â  Â  num_batches = 0
Â  Â Â 
Â  Â  pbar = tqdm(dataloader, desc=f"Epoch {epoch}")
Â  Â  for batch_idx, batch in enumerate(pbar):
Â  Â  Â  Â  poses = batch['poses'].to(device)
Â  Â  Â  Â  pose_mask = batch['pose_mask'].to(device)
Â  Â  Â  Â  text_tokens = batch['text_tokens'].to(device)
Â  Â  Â  Â  attention_mask = batch['attention_mask'].to(device)
Â  Â  Â  Â  seq_lengths = batch['seq_lengths'].to(device)
Â  Â  Â  Â Â 
Â  Â  Â  Â  batch_dict = {
Â  Â  Â  Â  Â  Â  'text_tokens': text_tokens,
Â  Â  Â  Â  Â  Â  'attention_mask': attention_mask,
Â  Â  Â  Â  Â  Â  'seq_lengths': seq_lengths,
Â  Â  Â  Â  Â  Â  'target_length': seq_lengths
Â  Â  Â  Â  }
Â  Â  Â  Â Â 
Â  Â  Â  Â  with torch.no_grad():
Â  Â  Â  Â  Â  Â  gt_latent = encoder(poses, mask=pose_mask)
Â  Â  Â  Â  Â  Â  gt_latent = gt_latent * scale_factorÂ 

Â  Â  Â  Â  return_attn = (batch_idx % log_attn_freq == 0)
Â  Â  Â  Â Â 
Â  Â  Â  Â  losses = flow_matcher(
Â  Â  Â  Â  Â  Â  batch_dict,Â 
Â  Â  Â  Â  Â  Â  gt_latent=gt_latent,Â 
Â  Â  Â  Â  Â  Â  pose_gt=poses,Â 
Â  Â  Â  Â  Â  Â  mode='train',Â 
Â  Â  Â  Â  Â  Â  return_attn_weights=return_attn
Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  optimizer.zero_grad()
Â  Â  Â  Â  losses['total'].backward()
Â  Â  Â  Â  torch.nn.utils.clip_grad_norm_(flow_matcher.parameters(), max_norm=1.0)
Â  Â  Â  Â  optimizer.step()
Â  Â  Â  Â  scheduler.step(epoch + batch_idx / len(dataloader))Â 
Â  Â  Â  Â Â 
Â  Â  Â  Â  total_loss += losses['total'].item()
Â  Â  Â  Â  losses_log['flow'] += losses['flow'].item()
Â  Â  Â  Â  losses_log['sync'] += losses.get('sync', torch.tensor(0.0)).item()
Â  Â  Â  Â  num_batches += 1
Â  Â  Â  Â Â 
Â  Â  Â  Â  pbar.set_postfix({'loss': f"{losses['total'].item():.4f}",Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'flow': f"{losses['flow'].item():.4f}",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'sync': f"{losses.get('sync', torch.tensor(0.0)).item():.4f}",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'lr': f"{optimizer.param_groups[0]['lr']:.2e}"})
Â  Â Â 
Â  Â  return {k: v / num_batches for k, v in losses_log.items()}, total_loss / num_batches

# === 3. HÃ€M VALIDATE (OPTIMIZED) ===
def validate(flow_matcher, encoder, dataloader, device, scale_factor):
Â  Â  flow_matcher.eval()
Â  Â  encoder.eval()
Â  Â  total_loss = 0.0
Â  Â  num_batches = 0
Â  Â Â 
Â  Â  # FIXED: Sá»­ dá»¥ng torch.no_grad() hoÃ n toÃ n vÃ¬ khÃ´ng cÃ²n tÃ­nh sync_grad
Â  Â  with torch.no_grad():
Â  Â  Â  Â  pbar = tqdm(dataloader, desc="Validating")
Â  Â  Â  Â  for batch in pbar:
Â  Â  Â  Â  Â  Â  poses = batch['poses'].to(device)
Â  Â  Â  Â  Â  Â  pose_mask = batch['pose_mask'].to(device)
Â  Â  Â  Â  Â  Â  text_tokens = batch['text_tokens'].to(device)
Â  Â  Â  Â  Â  Â  attention_mask = batch['attention_mask'].to(device)
Â  Â  Â  Â  Â  Â  seq_lengths = batch['seq_lengths'].to(device)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  batch_dict = {
Â  Â  Â  Â  Â  Â  Â  Â  'text_tokens': text_tokens,Â 
Â  Â  Â  Â  Â  Â  Â  Â  'attention_mask': attention_mask,Â 
Â  Â  Â  Â  Â  Â  Â  Â  'seq_lengths': seq_lengths
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  gt_latent = encoder(poses, mask=pose_mask)
Â  Â  Â  Â  Â  Â  gt_latent = gt_latent * scale_factorÂ 

Â  Â  Â  Â  Â  Â  losses = flow_matcher(
Â  Â  Â  Â  Â  Â  Â  Â  batch_dict,Â 
Â  Â  Â  Â  Â  Â  Â  Â  gt_latent=gt_latent,Â 
Â  Â  Â  Â  Â  Â  Â  Â  pose_gt=poses,Â 
Â  Â  Â  Â  Â  Â  Â  Â  mode='train' # Váº«n mode train Ä‘á»ƒ láº¥y loss
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  total_loss += losses['total'].item()
Â  Â  Â  Â  Â  Â  num_batches += 1
Â  Â  Â  Â  Â  Â Â 
Â  Â  return total_loss / num_batches if num_batches > 0 else 0.0

# === 4. MAIN ===
def main():
Â  Â  parser = argparse.ArgumentParser()
Â  Â  parser.add_argument('--data_dir', type=str, required=True)
Â  Â  parser.add_argument('--output_dir', type=str, required=True)
Â  Â  parser.add_argument('--autoencoder_checkpoint', type=str, required=True)
Â  Â  parser.add_argument('--batch_size', type=int, default=32)
Â  Â  parser.add_argument('--num_epochs', type=int, default=300)Â 
Â  Â  parser.add_argument('--learning_rate', type=float, default=1e-4)
Â  Â  parser.add_argument('--num_workers', type=int, default=4)
Â  Â  parser.add_argument('--latent_dim', type=int, default=256)
Â  Â  parser.add_argument('--max_seq_len', type=int, default=120)
Â  Â  parser.add_argument('--ae_hidden_dim', type=int, default=512)
Â  Â  parser.add_argument('--hidden_dim', type=int, default=512)
Â  Â  parser.add_argument('--num_layers', type=int, default=6)
Â  Â  parser.add_argument('--num_heads', type=int, default=8)
Â  Â  parser.add_argument('--dropout', type=float, default=0.1)
Â  Â  parser.add_argument('--use_ssm_prior', action='store_true', default=True)Â 
Â  Â  parser.add_argument('--use_sync_guidance', action='store_true', default=True)Â 
Â  Â Â 
Â  Â  parser.add_argument('--W_PRIOR', type=float, default=0.1)
Â  Â  parser.add_argument('--W_SYNC', type=float, default=0.5)
Â  Â Â 
Â  Â  parser.add_argument('--lambda_prior', type=float, default=0.1)
Â  Â  parser.add_argument('--gamma_guidance', type=float, default=0.1)
Â  Â  parser.add_argument('--resume_from', type=str, default=None)
Â  Â  parser.add_argument('--log_attn_freq', type=int, default=100)Â 

Â  Â  args = parser.parse_args()
Â  Â Â 
Â  Â  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
Â  Â  os.makedirs(args.output_dir, exist_ok=True)
Â  Â Â 
Â  Â  print(f"ğŸš€ Báº¯t Ä‘áº§u training SOTA Flow Matching trÃªn: {device}")

Â  Â  # Load AE
Â  Â  print("ğŸ”„ Loading Autoencoder...")
Â  Â  autoencoder = UnifiedPoseAutoencoder(pose_dim=214, latent_dim=args.latent_dim, hidden_dim=args.ae_hidden_dim)
Â  Â  checkpoint = torch.load(args.autoencoder_checkpoint, map_location=device)
Â  Â  if 'model_state_dict' in checkpoint: state_dict = checkpoint['model_state_dict']
Â  Â  else: state_dict = checkpoint
Â  Â  autoencoder.load_state_dict(state_dict)
Â  Â  encoder = autoencoder.encoder.to(device).eval().requires_grad_(False)

Â  Â  # Dataset
Â  Â  print("ğŸ“š Loading Dataset...")
Â  Â  train_dataset = SignLanguageDataset(args.data_dir, split='train', max_seq_len=args.max_seq_len)
Â  Â  val_dataset = SignLanguageDataset(args.data_dir, split='dev', max_seq_len=args.max_seq_len)
Â  Â  train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn)
Â  Â  val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers, collate_fn=collate_fn)
Â  Â Â 
Â  Â  # Scale Factor
Â  Â  latent_scale_factor = estimate_scale_factor(encoder, train_loader, device)

Â  Â  # Model
Â  Â  print("ğŸ”§ Init SOTA Flow Matcher...")
Â  Â  flow_matcher = LatentFlowMatcher(
Â  Â  Â  Â  latent_dim=args.latent_dim,
Â  Â  Â  Â  hidden_dim=args.hidden_dim,
Â  Â  Â  Â  num_flow_layers=args.num_layers,
Â  Â  Â  Â  num_prior_layers=args.num_layers,
Â  Â  Â  Â  num_heads=args.num_heads,
Â  Â  Â  Â  dropout=args.dropout,
Â  Â  Â  Â  use_ssm_prior=args.use_ssm_prior,
Â  Â  Â  Â  use_sync_guidance=args.use_sync_guidance,
Â  Â  Â  Â  lambda_prior=args.lambda_prior,
Â  Â  Â  Â  gamma_guidance=args.gamma_guidance,
Â  Â  Â  Â  lambda_anneal=True,
Â  Â  Â  Â  W_PRIOR=args.W_PRIOR,
Â  Â  Â  Â  W_SYNC=args.W_SYNC
Â  Â  ).to(device)
Â  Â Â 
Â  Â  # Optimizer
Â  Â  optimizer = torch.optim.AdamW(flow_matcher.parameters(), lr=args.learning_rate, weight_decay=0.05)
Â  Â Â 
Â  Â  scheduler = CosineAnnealingWarmRestarts(
Â  Â  Â  Â  optimizer,
Â  Â  Â  Â  T_0=20,Â 
Â  Â  Â  Â  T_mult=2,
Â  Â  Â  Â  eta_min=1e-6
Â  Â  )
Â  Â Â 
Â  Â  # Resume
Â  Â  start_epoch = 0
Â  Â  best_val_loss = float('inf')

Â  Â  if args.resume_from is None:
Â  Â  Â  Â  potential_latest = os.path.join(args.output_dir, 'latest.pt')
Â  Â  Â  Â  if os.path.exists(potential_latest):
Â  Â  Â  Â  Â  Â  args.resume_from = potential_latest
Â  Â  Â  Â  Â  Â  print(f"ğŸ” Tá»± Ä‘á»™ng tÃ¬m tháº¥y checkpoint: {args.resume_from}")

Â  Â  if args.resume_from and os.path.exists(args.resume_from):
Â  Â  Â  Â  print(f"â™»ï¸ Resuming from {args.resume_from}...")
Â  Â  Â  Â  ckpt = torch.load(args.resume_from, map_location=device)
Â  Â  Â  Â  flow_matcher.load_state_dict(ckpt['model_state_dict'])
Â  Â  Â  Â  if 'optimizer_state_dict' in ckpt: optimizer.load_state_dict(ckpt['optimizer_state_dict'])
Â  Â  Â  Â  if 'scheduler_state_dict' in ckpt: scheduler.load_state_dict(ckpt['scheduler_state_dict'])
Â  Â  Â  Â  start_epoch = ckpt.get('epoch', -1) + 1
Â  Â  Â  Â  best_val_loss = ckpt.get('best_val_loss', float('inf'))
Â  Â  Â  Â  latent_scale_factor = ckpt.get('latent_scale_factor', latent_scale_factor)
Â  Â  Â  Â  print(f"â© Resuming at Epoch: {start_epoch}, Best Loss: {best_val_loss:.4f}")

Â  Â  # Training Loop
Â  Â  for epoch in range(start_epoch, args.num_epochs):
Â  Â  Â  Â  train_metrics, avg_train_loss = train_epoch(flow_matcher, encoder, train_loader, optimizer, scheduler, device, epoch, latent_scale_factor, args.log_attn_freq)
Â  Â  Â  Â  val_loss = validate(flow_matcher, encoder, val_loader, device, latent_scale_factor)
Â  Â  Â  Â Â 
Â  Â  Â  Â  print(f"Epoch {epoch+1} | Train: {avg_train_loss:.6f} | Val: {val_loss:.6f} | Flow: {train_metrics['flow']:.6f} | Sync: {train_metrics['sync']:.6f} | LR: {optimizer.param_groups[0]['lr']:.2e}")
Â  Â  Â  Â Â 
Â  Â  Â  Â  save_dict = {
Â  Â  Â  Â  Â  Â  'epoch': epoch,
Â  Â  Â  Â  Â  Â  'model_state_dict': flow_matcher.state_dict(),
Â  Â  Â  Â  Â  Â  'optimizer_state_dict': optimizer.state_dict(),
Â  Â  Â  Â  Â  Â  'scheduler_state_dict': scheduler.state_dict(),
Â  Â  Â  Â  Â  Â  'latent_scale_factor': latent_scale_factor,
Â  Â  Â  Â  Â  Â  'best_val_loss': best_val_loss
Â  Â  Â  Â  }
Â  Â  Â  Â  torch.save(save_dict, os.path.join(args.output_dir, 'latest.pt'))
Â  Â  Â  Â Â 
Â  Â  Â  Â  if val_loss < best_val_loss:
Â  Â  Â  Â  Â  Â  best_val_loss = val_loss
Â  Â  Â  Â  Â  Â  save_dict['best_val_loss'] = best_val_lossÂ 
Â  Â  Â  Â  Â  Â  torch.save(save_dict, os.path.join(args.output_dir, 'best_model.pt'))
Â  Â  Â  Â  Â  Â  print(f"ğŸ† New Best Model Saved!")
Â  Â  Â  Â Â 
Â  Â  Â  Â  if (epoch + 1) % 50 == 0:
Â  Â  Â  Â  Â  Â  torch.save(save_dict, os.path.join(args.output_dir, f'checkpoint_epoch_{epoch+1}.pt'))

if __name__ == '__main__':
Â  Â  main()