import numpy as np
import cv2
import argparse
from tqdm import tqdm

# --- 1. Äá»ŠNH NGHÄ¨A Káº¾T Ná»I XÆ¯Æ NG (TOPOLOGY) ---
# Dá»±a trÃªn cáº¥u trÃºc 75 Ä‘iá»ƒm (OpenPose/Sign Language Datasets)
# 0-8: Body, 9-24: Right Hand? (Cáº§n check ká»¹ dataset, Ä‘Ã¢y lÃ  cáº¥u trÃºc phá»• biáº¿n)
# Tuy nhiÃªn, Ä‘á»ƒ an toÃ n, ta váº½ theo cáº¥u trÃºc chuáº©n OpenPose 25 body + Hands

# Body chain (Ä‘Æ¡n giáº£n hÃ³a Ä‘á»ƒ khÃ´ng bá»‹ rá»‘i)
BODY_CONNECTIONS = [
    (0,1), (1,2), (2,3), (3,4),       # Left Arm
    (1,5), (5,6), (6,7),              # Right Arm
    (1,8), (8,9), (9,10), (10,11),    # Torso & Left Leg
    (8,12), (12,13), (13,14),         # Right Leg
    (0,15), (0,16), (15,17), (16,18)  # Face contour/Eyes
]

# Hand chains (Cáº¥u trÃºc bÃ n tay chuáº©n)
# Left Hand (báº¯t Ä‘áº§u tá»« Ä‘iá»ƒm cá»• tay - index tuá»³ dataset, thÆ°á»ng lÃ  ná»‘i tiáº¿p body)
# Giáº£ sá»­ cáº¥u trÃºc 75 Ä‘iá»ƒm: 0-14 (Body), 15-18 (Head), 
# 25-45 (Left Hand), 46-66 (Right Hand) -> ÄÃ¢y lÃ  giáº£ Ä‘á»‹nh phá»• biáº¿n.
# NHÆ¯NG code data_prep cá»§a báº¡n gá»™p chung. 
# TÃ´i sáº½ dÃ¹ng logic váº½ Ä‘oáº¡n tháº³ng liá»n ká» cho pháº§n tay Ä‘á»ƒ Ä‘áº£m báº£o hiá»‡n hÃ¬nh.

def get_skeleton_topology():
    edges = list(BODY_CONNECTIONS)
    # ThÃªm cÃ¡c Ä‘Æ°á»ng ná»‘i tay (dá»±a trÃªn index giáº£ Ä‘á»‹nh cho vector 75 Ä‘iá»ƒm)
    # Náº¿u topology sai, nÃ³ sáº½ ná»‘i lung tung, nhÆ°ng Ã­t nháº¥t sáº½ hiá»‡n hÃ¬nh Ä‘á»ƒ báº¡n debug.
    return edges

# --- 2. HÃ€M Xá»¬ LÃ Dá»® LIá»†U ---

def split_pose(pose_vector):
    """
    TÃ¡ch vector 214D -> Body (75,2) vÃ  Mouth (20,2)
    """
    # Body: 0-150 -> 75 Ä‘iá»ƒm x 2
    body = pose_vector[:150].reshape(-1, 2)
    # Mouth: 174-214 -> 20 Ä‘iá»ƒm x 2
    mouth = pose_vector[174:214].reshape(-1, 2)
    return body, mouth

def robust_normalize_to_canvas(points, W=512, H=512, padding=50):
    """
    Ká»¹ thuáº­t AUTO-ZOOM: Báº¥t cháº¥p input lÃ  sá»‘ nhá» (normalized) hay sá»‘ to,
    nÃ³ sáº½ scale vá» khung hÃ¬nh 512x512.
    """
    # Lá»c bá» cÃ¡c Ä‘iá»ƒm (0,0) hoáº·c gáº§n 0 Ä‘á»ƒ khÃ´ng bá»‹ nhiá»…u scale
    valid_mask = np.sum(np.abs(points), axis=1) > 0.01
    valid_points = points[valid_mask]

    # Náº¿u khÃ´ng cÃ³ Ä‘iá»ƒm nÃ o valid (frame Ä‘en), tráº£ vá» gá»‘c
    if len(valid_points) == 0:
        return points

    # TÃ¬m Min/Max cá»§a dá»¯ liá»‡u tháº­t
    min_x, min_y = np.min(valid_points, axis=0)
    max_x, max_y = np.max(valid_points, axis=0)

    # TÃ­nh width/height cá»§a skeleton
    skel_w = max_x - min_x
    skel_h = max_y - min_y

    # TrÃ¡nh chia cho 0
    if skel_w < 1e-6: skel_w = 1
    if skel_h < 1e-6: skel_h = 1

    # TÃ­nh tá»· lá»‡ scale Ä‘á»ƒ fit vÃ o khung hÃ¬nh (trá»« padding)
    scale_x = (W - 2 * padding) / skel_w
    scale_y = (H - 2 * padding) / skel_h
    scale = min(scale_x, scale_y) # Giá»¯ aspect ratio

    # CÃ´ng thá»©c: (Point - Min) * Scale + Padding + Center_Offset
    # Center offset Ä‘á»ƒ hÃ¬nh náº±m giá»¯a khung
    final_w = skel_w * scale
    final_h = skel_h * scale
    offset_x = padding + (W - 2*padding - final_w) / 2
    offset_y = padding + (H - 2*padding - final_h) / 2

    # Apply transform
    points_scaled = np.copy(points)
    points_scaled[valid_mask] = (valid_points - [min_x, min_y]) * scale + [offset_x, offset_y]
    
    # Nhá»¯ng Ä‘iá»ƒm invalid (0,0) gÃ¡n vá» -1 Ä‘á»ƒ khÃ´ng váº½
    points_scaled[~valid_mask] = -1 
    
    return points_scaled

# --- 3. HÃ€M Váº¼ (STYLE OPENPOSE) ---

def draw_pose(canvas, body, mouth, is_gt=True):
    # MÃ u sáº¯c (BGR)
    if is_gt:
        c_body = (0, 255, 0)     # Green
        c_hand = (0, 0, 255)     # Red
        c_face = (255, 255, 255) # White
        label = "GROUND TRUTH"
    else:
        c_body = (0, 255, 255)   # Yellow
        c_hand = (255, 0, 255)   # Purple
        c_face = (0, 165, 255)   # Orange
        label = "RECONSTRUCTION"

    # 1. Váº½ Body (Ná»‘i dÃ¢y)
    # LÆ°u Ã½: VÃ¬ topology dataset nÃ y khÃ¡ Ä‘áº·c thÃ¹, ta sáº½ váº½ cÃ¡c Ä‘iá»ƒm trÆ°á»›c
    # Äá»ƒ an toÃ n: Váº½ táº¥t cáº£ cÃ¡c Ä‘iá»ƒm body thÃ nh cháº¥m trÃ²n
    for i, pt in enumerate(body):
        if pt[0] < 0: continue # Bá» qua Ä‘iá»ƒm invalid
        
        # PhÃ¢n biá»‡t mÃ u tay vÃ  ngÆ°á»i (giáº£ Ä‘á»‹nh index > 20 lÃ  tay)
        color = c_hand if i > 20 else c_body
        cv2.circle(canvas, (int(pt[0]), int(pt[1])), 3, color, -1)
        
        # Thá»­ ná»‘i Ä‘iá»ƒm i vá»›i i+1 (Heuristic Ä‘Æ¡n giáº£n Ä‘á»ƒ táº¡o hÃ¬nh liá»n máº¡ch)
        # Chá»‰ ná»‘i náº¿u Ä‘iá»ƒm tiáº¿p theo cÅ©ng valid vÃ  khÃ´ng quÃ¡ xa (trÃ¡nh ná»‘i tá»« tay ná» sang chÃ¢n kia)
        if i + 1 < len(body):
            pt_next = body[i+1]
            if pt_next[0] > 0:
                dist = np.linalg.norm(pt - pt_next)
                if dist < 100: # NgÆ°á»¡ng khoáº£ng cÃ¡ch pixel
                    cv2.line(canvas, (int(pt[0]), int(pt[1])), (int(pt_next[0]), int(pt_next[1])), color, 2)

    # 2. Váº½ Mouth (Cháº¥m nhá»)
    for pt in mouth:
        if pt[0] < 0: continue
        cv2.circle(canvas, (int(pt[0]), int(pt[1])), 1, c_face, -1)

    # 3. Text Label
    cv2.putText(canvas, label, (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, c_body, 2)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--gt_path", required=True)
    parser.add_argument("--recon_path", required=True)
    parser.add_argument("--output", default="comparison_fixed.mp4")
    args = parser.parse_args()

    # Load Data
    gt_seq = np.load(args.gt_path)
    recon_seq = np.load(args.recon_path)

    # Fix shape 1 frame
    if gt_seq.ndim == 1: gt_seq = gt_seq[None, :]
    if recon_seq.ndim == 1: recon_seq = recon_seq[None, :]

    length = min(len(gt_seq), len(recon_seq))
    
    # Init Video Writer
    H, W = 512, 512
    writer = cv2.VideoWriter(args.output, cv2.VideoWriter_fourcc(*'mp4v'), 25, (W*2, H))

    print(f"ğŸ¬ Processing {length} frames...")

    for t in tqdm(range(length)):
        # Táº¡o canvas Ä‘en
        frame_gt = np.zeros((H, W, 3), dtype=np.uint8)
        frame_recon = np.zeros((H, W, 3), dtype=np.uint8)

        # 1. TÃ¡ch Features
        b_gt, m_gt = split_pose(gt_seq[t])
        b_rc, m_rc = split_pose(recon_seq[t])

        # 2. AUTO-ZOOM (Critical Step)
        # Scale GT
        b_gt_s = robust_normalize_to_canvas(b_gt, W, H)
        m_gt_s = robust_normalize_to_canvas(m_gt, W, H) # LÆ°u Ã½: Mouth nÃªn scale theo Body Ä‘á»ƒ Ä‘Ãºng tá»‰ lá»‡
        
        # Scale Recon
        # Ta dÃ¹ng parameter scale cá»§a GT Ä‘á»ƒ Ã¡p dá»¥ng cho Recon -> Äá»ƒ so sÃ¡nh cÃ´ng báº±ng vá»‹ trÃ­
        # Tuy nhiÃªn, náº¿u recon nÃ¡t quÃ¡ thÃ¬ scale riÃªng. á» Ä‘Ã¢y scale riÃªng cho cháº¯c Äƒn hiá»ƒn thá»‹.
        b_rc_s = robust_normalize_to_canvas(b_rc, W, H)
        m_rc_s = robust_normalize_to_canvas(m_rc, W, H)

        # 3. Váº½
        draw_pose(frame_gt, b_gt_s, m_gt_s, is_gt=True)
        draw_pose(frame_recon, b_rc_s, m_rc_s, is_gt=False)

        # 4. Gá»™p vÃ  LÆ°u
        combined = np.hstack((frame_gt, frame_recon))
        writer.write(combined)

    writer.release()
    print(f"âœ… ÄÃ£ lÆ°u video táº¡i: {args.output}")

if __name__ == "__main__":
    main()