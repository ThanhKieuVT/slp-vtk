import numpy as np
import cv2
import argparse
import os
from tqdm import tqdm

# --- 1. B·∫¢N ƒê·ªí X∆Ø∆†NG CHU·∫®N (PHOENIX-14T / OPENPOSE) ---
# Body: 25 ƒëi·ªÉm (0-24)
BODY_EDGES = [
    (1, 0), (1, 2), (1, 5),   # C·ªï->M≈©i, Vai Ph·∫£i, Vai Tr√°i
    (2, 3), (3, 4),           # C√°nh tay ph·∫£i
    (5, 6), (6, 7),           # C√°nh tay tr√°i
    (1, 8), (8, 9), (8, 12),  # Th√¢n tr√™n -> H√¥ng
    (9, 10), (10, 11),        # Ch√¢n ph·∫£i
    (12, 13), (13, 14),       # Ch√¢n tr√°i
    (0, 15), (0, 16),         # M·∫Øt
    (15, 17), (16, 18)        # Tai
]

# Hand: 21 ƒëi·ªÉm (G·ªëc=0, Ng√≥n c√°i=1-4, Tr·ªè=5-8...)
HAND_EDGES = [
    (0, 1), (1, 2), (2, 3), (3, 4),      # Thumb
    (0, 5), (5, 6), (6, 7), (7, 8),      # Index
    (0, 9), (9, 10), (10, 11), (11, 12), # Middle
    (0, 13), (13, 14), (14, 15), (15, 16), # Ring
    (0, 17), (17, 18), (18, 19), (19, 20)  # Pinky
]

def auto_scale_pose(pose, W=512, H=512, padding=50):
    """
    T·ª± ƒë·ªông ph√≥ng to/thu nh·ªè pose ƒë·ªÉ v·ª´a kh√≠t khung h√¨nh 512x512
    """
    # L·ªçc b·ªè c√°c ƒëi·ªÉm (0,0) (ƒëi·ªÉm b·ªã che khu·∫•t/kh√¥ng c√≥)
    valid_mask = np.sum(pose, axis=1) != 0
    valid_points = pose[valid_mask]
    
    if len(valid_points) == 0:
        return pose # Kh√¥ng c√≥ ƒëi·ªÉm n√†o ƒë·ªÉ v·∫Ω

    # T√¨m h·ªôp bao (Bounding Box)
    min_x, min_y = np.min(valid_points, axis=0)
    max_x, max_y = np.max(valid_points, axis=0)
    
    pose_w = max_x - min_x
    pose_h = max_y - min_y
    
    # T√≠nh t·ªâ l·ªá scale ƒë·ªÉ fit v√†o khung h√¨nh (tr·ª´ padding)
    scale_x = (W - 2*padding) / (pose_w + 1e-6)
    scale_y = (H - 2*padding) / (pose_h + 1e-6)
    scale = min(scale_x, scale_y) # Gi·ªØ t·ªâ l·ªá khung h√¨nh (aspect ratio)
    
    # Scale v√† d·ªãch chuy·ªÉn v·ªÅ gi·ªØa
    pose_scaled = (pose - [min_x, min_y]) * scale
    
    # CƒÉn gi·ªØa
    new_w = pose_w * scale
    new_h = pose_h * scale
    offset_x = (W - new_w) / 2
    offset_y = (H - new_h) / 2
    
    return pose_scaled + [offset_x, offset_y]

def draw_pose_on_canvas(canvas, keypoints, is_gt=True):
    """
    V·∫Ω khung x∆∞∆°ng l√™n n·ªÅn ƒëen
    keypoints: [75, 2]
    """
    # T√°ch b·ªô ph·∫≠n (D·ª±a tr√™n c·∫•u tr√∫c 75 ƒëi·ªÉm: 25 Body + 21 LHand + 21 RHand)
    # L∆∞u √Ω: Index c√≥ th·ªÉ thay ƒë·ªïi t√πy b·ªô data, nh∆∞ng ƒë√¢y l√† c·∫•u tr√∫c ph·ªï bi·∫øn nh·∫•t
    body = keypoints[0:25]
    l_hand = keypoints[25:46]
    r_hand = keypoints[46:67]
    
    # M√†u s·∫Øc: GT (Xanh l√°), Recon (ƒê·ªè/Cam)
    color_body = (0, 255, 0) if is_gt else (0, 0, 255)       # Body
    color_lhand = (0, 200, 200) if is_gt else (0, 165, 255) # Tay Tr√°i (V√†ng/Cam)
    color_rhand = (200, 200, 0) if is_gt else (255, 0, 255) # Tay Ph·∫£i (Xanh l∆°/T√≠m)

    # H√†m v·∫Ω ƒë∆∞·ªùng n·ªëi
    def draw_lines(points, edges, color, thick=2):
        for u, v in edges:
            if u < len(points) and v < len(points):
                pt1 = tuple(points[u].astype(int))
                pt2 = tuple(points[v].astype(int))
                # Kh√¥ng v·∫Ω n·∫øu ƒëi·ªÉm l√† (0,0) ho·∫∑c bay ra ngo√†i khung
                if pt1 != (0,0) and pt2 != (0,0):
                    cv2.line(canvas, pt1, pt2, color, thick)
                    # V·∫Ω kh·ªõp tr√≤n nh·ªè
                    cv2.circle(canvas, pt1, 2, color, -1)

    draw_lines(body, BODY_EDGES, color_body, 2)
    draw_lines(l_hand, HAND_EDGES, color_lhand, 1)
    draw_lines(r_hand, HAND_EDGES, color_rhand, 1)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--gt_path', type=str, required=True, help='Path file g·ªëc .npy')
    parser.add_argument('--recon_path', type=str, required=True, help='Path file t√°i t·∫°o .npy')
    parser.add_argument('--output_video', type=str, default='comparison_video.mp4')
    args = parser.parse_args()
    
    # 1. Load Data
    print(f"üìÇ Loading: {args.gt_path}")
    gt_data = np.load(args.gt_path)
    print(f"üìÇ Loading: {args.recon_path}")
    recon_data = np.load(args.recon_path)
    
    # 2. X·ª≠ l√Ω ƒë·ªô d√†i l·ªách nhau (C·∫Øt theo c√°i ng·∫Øn nh·∫•t)
    len_gt = len(gt_data)
    len_recon = len(recon_data)
    min_len = min(len_gt, len_recon)
    
    if len_gt != len_recon:
        print(f"‚ö†Ô∏è C·∫£nh b√°o: ƒê·ªô d√†i l·ªách nhau (GT={len_gt}, Rec={len_recon}). S·∫Ω c·∫Øt v·ªÅ {min_len} frames.")
    
    # 3. Chu·∫©n b·ªã Video Writer
    H, W = 512, 512 # K√≠ch th∆∞·ªõc m·ªói khung h√¨nh
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    # Video ƒë·∫ßu ra s·∫Ω r·ªông g·∫•p ƒë√¥i (Side-by-side)
    out = cv2.VideoWriter(args.output_video, fourcc, 25, (W * 2, H))
    
    print(f"üé¨ ƒêang render video ({min_len} frames)...")
    
    # 4. V√≤ng l·∫∑p v·∫Ω
    for t in tqdm(range(min_len)):
        # L·∫•y data frame t & reshape v·ªÅ [75, 2] (Ch·ªâ l·∫•y 150 chi·ªÅu ƒë·∫ßu)
        pose_gt = gt_data[t, :150].reshape(-1, 2)
        pose_rec = recon_data[t, :150].reshape(-1, 2)
        
        # Auto Scale ƒë·ªÉ fit v√†o khung h√¨nh 512x512
        pose_gt_scaled = auto_scale_pose(pose_gt, W, H)
        pose_rec_scaled = auto_scale_pose(pose_rec, W, H)
        
        # T·∫°o canvas ƒëen
        canvas_gt = np.zeros((H, W, 3), dtype=np.uint8)
        canvas_rec = np.zeros((H, W, 3), dtype=np.uint8)
        
        # V·∫Ω
        draw_pose_on_canvas(canvas_gt, pose_gt_scaled, is_gt=True)
        draw_pose_on_canvas(canvas_rec, pose_rec_scaled, is_gt=False)
        
        # Th√™m nh√£n
        cv2.putText(canvas_gt, "GROUND TRUTH", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        cv2.putText(canvas_rec, "RECONSTRUCTED", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
        cv2.putText(canvas_gt, f"Frame: {t}/{min_len}", (20, H-20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        
        # Gh√©p 2 khung h√¨nh
        final_frame = np.hstack([canvas_gt, canvas_rec])
        out.write(final_frame)
        
    out.release()
    print(f"\n‚úÖ Xong! Video ƒë√£ l∆∞u t·∫°i: {args.output_video}")

if __name__ == '__main__':
    main()