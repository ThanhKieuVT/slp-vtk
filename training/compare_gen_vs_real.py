import os
import sys
import argparse
import torch
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib.lines import Line2D
from torch.utils.data import DataLoader

# Th√™m ƒë∆∞·ªùng d·∫´n project
sys.path.append(os.getcwd())

try:
    from dataset import SignLanguageDataset, collate_fn
    from models.fml.autoencoder import UnifiedPoseAutoencoder
    from models.fml.latent_flow_matcher import LatentFlowMatcher
except ImportError:
    sys.path.append(os.path.join(os.getcwd(), '..'))
    from dataset import SignLanguageDataset, collate_fn
    from models.autoencoder import UnifiedPoseAutoencoder
    from models.fml.latent_flow_matcher import LatentFlowMatcher

def denormalize(pose, mean, std):
    """ƒê∆∞a pose v·ªÅ gi√° tr·ªã g·ªëc"""
    return pose * std + mean

# --- TOPOLOGY CHU·∫®N C·ª¶A MEDIAPIPE HOLISTIC ---
# Body (Pose Landmarks 0-32)
# Ch√∫ √Ω: MediaPipe Pose c√≥ c·∫•u tr√∫c c·ª• th·ªÉ. Ta s·∫Ω v·∫Ω c√°c ƒë∆∞·ªùng ch√≠nh.
BODY_CONNECTIONS = [
    (11, 12), (11, 23), (12, 24), (23, 24), # Th√¢n
    (11, 13), (13, 15), (15, 21), (15, 17), (15, 19), (17, 19), # Tay tr√°i (c√°nh tay)
    (12, 14), (14, 16), (16, 22), (16, 18), (16, 20), (18, 20), # Tay ph·∫£i (c√°nh tay)
    (0, 1), (1, 2), (2, 3), (3, 7), (0, 4), (4, 5), (5, 6), (6, 8), # M·∫∑t (s∆° b·ªô)
    (9, 10) # Mi·ªáng (s∆° b·ªô)
]

# Hand (0-20) - Chu·∫©n MediaPipe Hand
HAND_CONNECTIONS = [
    (0, 1), (1, 2), (2, 3), (3, 4),         # Thumb
    (0, 5), (5, 6), (6, 7), (7, 8),         # Index
    (5, 9), (9, 10), (10, 11), (11, 12),    # Middle
    (9, 13), (13, 14), (14, 15), (15, 16),  # Ring
    (13, 17), (17, 18), (18, 19), (19, 20), # Pinky
    (0, 17) # Palm base
]

# Mouth (20 points from Face Mesh) - Ta v·∫Ω v√≤ng tr√≤n
MOUTH_CONNECTIONS = list(zip(range(0, 19), range(1, 20))) + [(19, 0)]

# --- T·ªîNG H·ª¢P K·∫æT N·ªêI CHO VISUALIZER ---
ALL_CONNECTIONS = []

# 1. Body (Indices 0-32)
ALL_CONNECTIONS.extend([
    {'indices': (s, e), 'offset': 0, 'color': 'gray', 'lw': 2}
    for (s, e) in BODY_CONNECTIONS
])

# 2. Left Hand (Indices 33-53) -> Offset 33
ALL_CONNECTIONS.extend([
    {'indices': (s, e), 'offset': 33, 'color': 'green', 'lw': 1.5}
    for (s, e) in HAND_CONNECTIONS
])
# N·ªëi c·ªï tay tr√°i (Body 15) v·ªõi g·ªëc b√†n tay tr√°i (Hand 0 -> idx 33)
ALL_CONNECTIONS.append({'indices': (15, 33), 'offset': 0, 'color': 'green', 'lw': 2, 'type': 'link'})

# 3. Right Hand (Indices 54-74) -> Offset 54
ALL_CONNECTIONS.extend([
    {'indices': (s, e), 'offset': 54, 'color': 'blue', 'lw': 1.5}
    for (s, e) in HAND_CONNECTIONS
])
# N·ªëi c·ªï tay ph·∫£i (Body 16) v·ªõi g·ªëc b√†n tay ph·∫£i (Hand 0 -> idx 54)
ALL_CONNECTIONS.append({'indices': (16, 54), 'offset': 0, 'color': 'blue', 'lw': 2, 'type': 'link'})

# 4. Mouth (Indices 174-213 -> 40 values -> 20 points) -> Offset trong array v·∫Ω (sau khi l·ªçc)
# L∆∞u √Ω: Ta s·∫Ω x·ª≠ l√Ω ri√™ng ph·∫ßn Mouth v√¨ n√≥ n·∫±m t√≠t ·ªü index 174 (c√°ch xa ƒë√°m tr√™n)

# --- INDICES ƒê·ªÇ V·∫º ---
# Ta s·∫Ω t·∫°o m·ªôt array r√∫t g·ªçn ch·ªâ ch·ª©a c√°c ƒëi·ªÉm to·∫° ƒë·ªô (x,y) ƒë·ªÉ v·∫Ω
# 1. Body: 0-32 (33 points)
# 2. LHand: 33-53 (21 points)
# 3. RHand: 54-74 (21 points)
# --- SKIP 150-173 (Non-coordinate features) ---
# 4. Mouth: 174-213 (20 points x 2)

VALID_POINT_THRESHOLD = 0.01 # L·ªçc ƒëi·ªÉm (0,0)

class CorrectSkeletonVisualizer:
    def prepare_data(self, pose_214):
        """
        Input: [T, 214]
        Output: [T, 95, 2] containing only coordinates (Body+Hands+Mouth)
        """
        T = pose_214.shape[0]
        
        # 1. Manual Parts (Body + Hands): Index 0-149 -> 75 points
        manual_part = pose_214[:, :150].reshape(T, 75, 2)
        
        # 2. Mouth Part: Index 174-213 -> 40 values -> 20 points
        mouth_part = pose_214[:, 174:].reshape(T, 20, 2)
        
        # Gh√©p l·∫°i: 75 + 20 = 95 points
        # Index m·ªõi:
        # 0-32: Body
        # 33-53: LHand
        # 54-74: RHand
        # 75-94: Mouth
        clean_pose = np.concatenate([manual_part, mouth_part], axis=1)
        return clean_pose

    def create_animation(self, real_pose_raw, gen_pose_raw, text, save_path):
        # 1. Prepare Data
        real_kps = self.prepare_data(real_pose_raw)
        gen_kps = self.prepare_data(gen_pose_raw)
        T = len(real_kps)
        
        # 2. Setup Figure
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))
        fig.suptitle(f"Text: {text[:60]}...", fontsize=12)
        ax1.set_title("Ground Truth")
        ax2.set_title("Generated (Flow)")

        # T√≠nh gi·ªõi h·∫°n khung h√¨nh (D·ª±a tr√™n Body points ƒë·ªÉ ·ªïn ƒë·ªãnh)
        body_points = real_kps[:, :33] # Ch·ªâ l·∫•y body ƒë·ªÉ t√≠nh scale
        valid_mask = np.sum(np.abs(body_points), axis=2) > VALID_POINT_THRESHOLD
        if valid_mask.any():
            valid_vals = body_points[valid_mask]
            min_vals = np.min(valid_vals, axis=0)
            max_vals = np.max(valid_vals, axis=0)
            pad = 0.1
            
            for ax in [ax1, ax2]:
                ax.set_xlim(min_vals[0] - pad, max_vals[0] + pad)
                ax.set_ylim(max_vals[1] + pad, min_vals[1] - pad) # Invert Y
                ax.set_aspect('equal')
                ax.axis('off')
        else:
            for ax in [ax1, ax2]:
                ax.set_xlim(0, 1); ax.set_ylim(1, 0); ax.axis('off')

        # 3. Setup Lines
        def init_lines(ax):
            lines = []
            
            # Body & Hands
            for item in ALL_CONNECTIONS:
                line = Line2D([], [], color=item['color'], lw=item['lw'], alpha=0.8)
                ax.add_line(line)
                lines.append({'line': line, 'item': item})
            
            # Mouth (Offset 75 trong m·∫£ng clean_pose)
            for (s, e) in MOUTH_CONNECTIONS:
                line = Line2D([], [], color='red', lw=1.5, alpha=0.8)
                ax.add_line(line)
                lines.append({'line': line, 'item': {'indices': (s, e), 'offset': 75}})
                
            return lines

        lines1 = init_lines(ax1)
        lines2 = init_lines(ax2)

        def update_frame(kps_frame, lines_list):
            for obj in lines_list:
                item = obj['item']
                line = obj['line']
                (s, e) = item['indices']
                
                # T√≠nh index th·ª±c t·∫ø
                if 'type' in item and item['type'] == 'link':
                    # Tr∆∞·ªùng h·ª£p n·ªëi ƒë·∫∑c bi·ªát (vd: C·ªï tay -> B√†n tay)
                    # item['indices'] l√† index tuy·ªát ƒë·ªëi trong m·∫£ng 95
                    idx_start, idx_end = s, e
                else:
                    # Tr∆∞·ªùng h·ª£p offset th∆∞·ªùng
                    offset = item['offset']
                    idx_start, idx_end = s + offset, e + offset
                
                p1 = kps_frame[idx_start]
                p2 = kps_frame[idx_end]
                
                # Check threshold (0,0)
                if np.sum(np.abs(p1)) > VALID_POINT_THRESHOLD and np.sum(np.abs(p2)) > VALID_POINT_THRESHOLD:
                    line.set_data([p1[0], p2[0]], [p1[1], p2[1]])
                else:
                    line.set_data([], [])
            return [obj['line'] for obj in lines_list]

        def update(frame):
            artists1 = update_frame(real_kps[frame], lines1)
            idx_gen = min(frame, len(gen_kps) - 1)
            artists2 = update_frame(gen_kps[idx_gen], lines2)
            return artists1 + artists2

        ani = animation.FuncAnimation(fig, update, frames=T, blit=True, interval=40)
        ani.save(save_path, writer='ffmpeg', fps=25)
        plt.close()

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--text_file", type=str, required=True)
    parser.add_argument("--data_dir", type=str, required=True)
    parser.add_argument("--split", type=str, default="test")
    parser.add_argument("--flow_ckpt", type=str, required=True)
    parser.add_argument("--ae_ckpt", type=str, required=True)
    parser.add_argument("--num_samples", type=int, default=5)
    parser.add_argument("--output_dir", type=str, default="eval_results_clean")
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--latent_dim", type=int, default=256)
    parser.add_argument("--hidden_dim", type=int, default=512)
    parser.add_argument("--ae_hidden_dim", type=int, default=512)
    
    args = parser.parse_args()
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    os.makedirs(args.output_dir, exist_ok=True)
    
    print(f"üöÄ Evaluation started (Correct Topology)...")

    # Load Stats
    stats_path = os.path.join(args.data_dir, "normalization_stats.npz")
    if not os.path.exists(stats_path):
        mean, std = 0, 1
    else:
        stats = np.load(stats_path)
        mean = stats['mean']
        std = stats['std']
    
    dataset = SignLanguageDataset(
        data_dir=args.data_dir,
        split=args.split,
        text_file=args.text_file,
        max_seq_len=200
    )
    
    ae = UnifiedPoseAutoencoder(latent_dim=args.latent_dim, hidden_dim=args.ae_hidden_dim).to(device)
    ae.load_state_dict(torch.load(args.ae_ckpt, map_location=device)['model_state_dict'])
    ae.eval()
    
    flow_matcher = LatentFlowMatcher(latent_dim=args.latent_dim, hidden_dim=args.hidden_dim).to(device)
    flow_ckpt = torch.load(args.flow_ckpt, map_location=device)
    flow_matcher.load_state_dict(flow_ckpt['model_state_dict'], strict=False)
    flow_matcher.eval()
    
    latent_scale = float(flow_ckpt.get("latent_scale_factor", 1.0))
    print(f"üìè Scale: {latent_scale:.4f}")

    visualizer = CorrectSkeletonVisualizer()

    indices = np.random.choice(len(dataset), size=min(len(dataset), args.num_samples), replace=False)
    subset = torch.utils.data.Subset(dataset, indices)
    loader = DataLoader(subset, batch_size=1, collate_fn=collate_fn)
    
    print(f"üé¨ Generating {len(subset)} samples...")
    
    for i, batch in enumerate(loader):
        video_id = batch['video_ids'][0]
        text = batch['texts'][0]
        pose_gt = batch['poses'][0].cpu().numpy()
        
        text_tokens = batch['text_tokens'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        
        text_features, text_mask = flow_matcher.encode_text(text_tokens, attention_mask)
        
        gen_latent = flow_matcher._inference_forward(
            batch=None, 
            text_features=text_features, 
            text_mask=text_mask,
            num_steps=50
        )
        
        gen_latent_scaled = gen_latent * latent_scale
        gen_pose = ae.decode(gen_latent_scaled)
        gen_pose = gen_pose.squeeze(0).detach().cpu().numpy()
        
        real_pose_denorm = denormalize(pose_gt, mean, std)
        gen_pose_denorm = denormalize(gen_pose, mean, std)
        
        save_path_base = os.path.join(args.output_dir, f"sample_{i}_{video_id}")
        
        try:
            visualizer.create_animation(real_pose_denorm, gen_pose_denorm, text, f"{save_path_base}.mp4")
            print(f"   [{i+1}] üé• Video: {save_path_base}.mp4")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Render error: {e}")
            import traceback
            traceback.print_exc()

    print("‚úÖ Done!")

if __name__ == "__main__":
    main()