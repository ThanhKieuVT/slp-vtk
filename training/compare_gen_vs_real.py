import os
import argparse
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import sys

# Th√™m ƒë∆∞·ªùng d·∫´n ƒë·ªÉ import module c·ªßa ch·ªã
sys.path.append(os.getcwd())

try:
    from data_preparation import load_sample
    # Import c·∫•u tr√∫c x∆∞∆°ng ƒë·ªÉ v·∫Ω
    from visualize_single_pose import HAND_CONNECTIONS, POSE_CONNECTIONS_UPPER_BODY, FACE_CONNECTIONS
except ImportError:
    print("‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng import ƒë∆∞·ª£c c·∫•u tr√∫c x∆∞∆°ng. V·∫Ω point cloud ƒë∆°n gi·∫£n.")
    HAND_CONNECTIONS = []
    POSE_CONNECTIONS_UPPER_BODY = []
    FACE_CONNECTIONS = []

def visualize_comparison(real_pose, gen_pose, output_path, title_text=""):
    """
    T·∫°o video so s√°nh: B√™n tr√°i (Real) - B√™n ph·∫£i (Generated)
    """
    # C·∫Øt ng·∫Øn v·ªÅ ƒë·ªô d√†i chung nh·ªè nh·∫•t ƒë·ªÉ so s√°nh
    min_len = min(len(real_pose), len(gen_pose))
    real_pose = real_pose[:min_len]
    gen_pose = gen_pose[:min_len]
    
    print(f"üé¨ ƒêang t·∫°o video so s√°nh ({min_len} frames)...")
    
    # Setup Plot
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))
    
    # Config tr·ª•c (Gi·∫£ s·ª≠ pose ƒë√£ denormalize v·ªÅ pixel 256x256 ho·∫∑c t∆∞∆°ng t·ª±)
    # Ch·ªã c√≥ th·ªÉ c·∫ßn ch·ªânh limit n√†y t√πy theo scale data c·ªßa ch·ªã
    for ax in [ax1, ax2]:
        ax.set_xlim(0, 260) # Chi·ªÅu r·ªông ·∫£nh g·ªëc Phoenix
        ax.set_ylim(210, 0) # Chi·ªÅu cao ·∫£nh g·ªëc (ƒë·∫£o ng∆∞·ª£c tr·ª•c y)
        ax.axis('off')

    ax1.set_title("Ground Truth (Real)")
    ax2.set_title("Generated (AI)")
    fig.suptitle(f"Compare: {title_text}", fontsize=10)

    # Init artists
    lines_real = []
    lines_gen = []
    scats_real = []
    scats_gen = []

    # H√†m v·∫Ω helper
    def init_skeleton(ax, collection_lines, collection_scats):
        # V·∫Ω body (Line)
        for _ in range(len(POSE_CONNECTIONS_UPPER_BODY) + len(HAND_CONNECTIONS)):
            line, = ax.plot([], [], 'k-', linewidth=1) # ƒêen
            collection_lines.append(line)
        # V·∫Ω face (Scatter cho nh·∫π)
        scat = ax.scatter([], [], s=2, c='r') # ƒê·ªè
        collection_scats.append(scat)

    init_skeleton(ax1, lines_real, scats_real)
    init_skeleton(ax2, lines_gen, scats_gen)

    def update(frame):
        # L·∫•y frame hi·ªán t·∫°i
        pose_r = real_pose[frame].reshape(-1, 2) # [214/2, 2]
        pose_g = gen_pose[frame].reshape(-1, 2)
        
        # Update function cho 1 b√™n
        def update_ax(pose_data, lines, scats):
            # T√°ch c√°c ph·∫ßn
            # Indices (Hardcode theo Mediapipe Holistic r√∫t g·ªçn c·ªßa ch·ªã)
            # Coarse: 0-33, Face: ...
            # ƒê·ªÉ ƒë∆°n gi·∫£n, v·∫Ω h·∫øt c√°c k·∫øt n·ªëi c√≥ s·∫µn
            
            line_idx = 0
            
            # 1. V·∫Ω Body & Hands (D√πng Line)
            # C·∫ßn map l·∫°i index t·ª´ 214 vector sang index c·ªßa skeleton map
            # Gi·∫£ s·ª≠ pose_data ƒë√£ ƒë√∫ng th·ª© t·ª± extraction
            
            # Note: Ch·ªã c·∫ßn ƒë·∫£m b·∫£o index trong VISUALIZE_SINGLE_POSE kh·ªõp v·ªõi data 214
            # ·ªû ƒë√¢y em v·∫Ω point cloud n·∫øu map kh√¥ng kh·ªõp, ho·∫∑c th·ª≠ v·∫Ω line c∆° b·∫£n
            
            # V·∫Ω Line Body
            for i, (start, end) in enumerate(POSE_CONNECTIONS_UPPER_BODY):
                if start < len(pose_data) and end < len(pose_data):
                    if pose_data[start].sum() != 0 and pose_data[end].sum() != 0:
                        lines[line_idx].set_data([pose_data[start, 0], pose_data[end, 0]],
                                                 [pose_data[start, 1], pose_data[end, 1]])
                    else:
                        lines[line_idx].set_data([], [])
                    line_idx += 1
            
            # V·∫Ω Hands
            # (C·∫ßn offset index n·∫øu tay n·∫±m sau body trong m·∫£ng 214)
            # T·∫°m th·ªùi v·∫Ω scatter to√†n b·ªô cho ch·∫Øc ƒÉn n·∫øu index lo·∫°n
            scats[0].set_offsets(pose_data) # V·∫Ω t·∫•t c·∫£ ƒëi·ªÉm d·∫°ng ch·∫•m ƒë·ªè

        update_ax(pose_r, lines_real, scats_real)
        update_ax(pose_g, lines_gen, scats_gen)
        
        return lines_real + scats_real + lines_gen + scats_gen

    ani = animation.FuncAnimation(fig, update, frames=min_len, blit=True, interval=40)
    ani.save(output_path, writer='ffmpeg', fps=25)
    print(f"‚úÖ Xong! Video l∆∞u t·∫°i: {output_path}")
    plt.close()

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--gen_path', type=str, required=True, help="File .npy sinh ra t·ª´ inference")
    parser.add_argument('--data_dir', type=str, required=True, help="Th∆∞ m·ª•c ch·ª©a data g·ªëc (processed_data/data)")
    parser.add_argument('--video_id', type=str, required=True, help="ID c·ªßa video g·ªëc (VD: 27January...)")
    parser.add_argument('--split', type=str, default='train', help="Video g·ªëc n·∫±m ·ªü split n√†o (train/dev/test)")
    parser.add_argument('--output_video', type=str, default='compare.mp4')
    args = parser.parse_args()

    # 1. Load Generated Pose
    print(f"üìÇ Loading Gen: {args.gen_path}")
    gen_pose = np.load(args.gen_path)
    
    # 2. Load Real Pose
    split_dir = os.path.join(args.data_dir, args.split)
    print(f"üìÇ Loading Real ID: {args.video_id} t·ª´ {split_dir}")
    
    # D√πng h√†m load_sample c√≥ s·∫µn c·ªßa ch·ªã ƒë·ªÉ load chu·∫©n
    real_pose, T = load_sample(args.video_id, split_dir)
    
    if real_pose is None:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file pose g·ªëc! Ki·ªÉm tra l·∫°i ID ho·∫∑c Split.")
        sys.exit(1)
        
    # 3. So s√°nh
    print(f"üìä Stats:")
    print(f"   - Gen Shape: {gen_pose.shape}")
    print(f"   - Real Shape: {real_pose.shape}")
    
    # T·∫°o video
    visualize_comparison(real_pose, gen_pose, args.output_video, args.video_id)

if __name__ == '__main__':
    main()