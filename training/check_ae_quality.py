"""
Script: Check Autoencoder Reconstruction Quality (Stage 1)
âœ… FIXED: TÆ°Æ¡ng thÃ­ch vá»›i grouped normalization (manual + NMM)
Má»¥c Ä‘Ã­ch: Kiá»ƒm tra cháº¥t lÆ°á»£ng tÃ¡i táº¡o cá»§a Autoencoder
"""

import torch
import numpy as np
import argparse
import os
import sys

# Import modules
try:
    from models.fml.autoencoder import UnifiedPoseAutoencoder
    from data_preparation import load_sample, normalize_pose, denormalize_pose
except ImportError as e:
    print(f"âŒ Lá»—i Import: {e}")
    print("ğŸ’¡ Äáº£m báº£o file nÃ y á»Ÿ thÆ° má»¥c gá»‘c, cÃ¹ng cáº¥p vá»›i 'models/' vÃ  'data_preparation.py'")
    sys.exit(1)


def main():
    parser = argparse.ArgumentParser(description='Kiá»ƒm tra Autoencoder Stage 1')
    
    parser.add_argument('--data_dir', type=str, required=True,
                        help='ÄÆ°á»ng dáº«n Ä‘áº¿n processed_data/data/')
    parser.add_argument('--autoencoder_checkpoint', type=str, required=True,
                        help='Checkpoint cá»§a Autoencoder (.pt)')
    parser.add_argument('--output_dir', type=str, default='check_stage1_output',
                        help='ThÆ° má»¥c lÆ°u káº¿t quáº£')
    parser.add_argument('--split', type=str, default='dev',
                        choices=['train', 'dev', 'test'],
                        help='Split Ä‘á»ƒ láº¥y máº«u test')
    parser.add_argument('--sample_idx', type=int, default=0,
                        help='Chá»‰ sá»‘ máº«u Ä‘á»ƒ test (default: máº«u Ä‘áº§u tiÃªn)')
    
    # Model config (pháº£i khá»›p vá»›i lÃºc train)
    parser.add_argument('--latent_dim', type=int, default=256)
    parser.add_argument('--hidden_dim', type=int, default=512)
    parser.add_argument('--encoder_layers', type=int, default=6)
    parser.add_argument('--decoder_coarse_layers', type=int, default=4)
    parser.add_argument('--decoder_medium_layers', type=int, default=4)
    parser.add_argument('--decoder_fine_layers', type=int, default=6)
    parser.add_argument('--num_heads', type=int, default=8)
    
    args = parser.parse_args()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ Device: {device}")
    os.makedirs(args.output_dir, exist_ok=True)
    
    # ============================================================
    # 1. LOAD GROUPED NORMALIZATION STATS
    # ============================================================
    stats_path = os.path.join(args.data_dir, "normalization_stats.npz")
    if not os.path.exists(stats_path):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y stats táº¡i: {stats_path}")
        print("ğŸ’¡ Cháº¡y data_preparation.py Ä‘á»ƒ táº¡o stats trÆ°á»›c!")
        return
    
    print(f"ğŸ“Š Loading grouped stats tá»«: {stats_path}")
    stats_data = np.load(stats_path)
    stats = {
        'manual_mean': float(stats_data['manual_mean']),
        'manual_std': float(stats_data['manual_std']),
        'nmm_mean': stats_data['nmm_mean'],
        'nmm_std': stats_data['nmm_std']
    }
    print(f"   âœ… Manual: mean={stats['manual_mean']:.4f}, std={stats['manual_std']:.4f}")
    print(f"   âœ… NMM: shape={stats['nmm_mean'].shape}")
    
    # ============================================================
    # 2. LOAD AUTOENCODER MODEL
    # ============================================================
    print(f"\nğŸ“¦ Loading Autoencoder tá»«: {args.autoencoder_checkpoint}")
    model = UnifiedPoseAutoencoder(
        pose_dim=214,
        latent_dim=args.latent_dim,
        hidden_dim=args.hidden_dim,
        encoder_layers=args.encoder_layers,
        decoder_coarse_layers=args.decoder_coarse_layers,
        decoder_medium_layers=args.decoder_medium_layers,
        decoder_fine_layers=args.decoder_fine_layers,
        num_heads=args.num_heads,
        dropout=0.1
    ).to(device)
    
    try:
        checkpoint = torch.load(args.autoencoder_checkpoint, map_location=device)
        state_dict = checkpoint.get('model_state_dict', checkpoint)
        model.load_state_dict(state_dict, strict=True)
        print("   âœ… Load weights thÃ nh cÃ´ng!")
        
        # In thÃ´ng tin checkpoint
        if 'epoch' in checkpoint:
            print(f"   ğŸ“Œ Epoch: {checkpoint['epoch']}")
        if 'val_loss' in checkpoint:
            print(f"   ğŸ“Œ Val Loss: {checkpoint['val_loss']:.6f}")
        
        model.eval()
    except Exception as e:
        print(f"âŒ Lá»—i load checkpoint: {e}")
        return
    
    # ============================================================
    # 3. LOAD SAMPLE DATA
    # ============================================================
    split_dir = os.path.join(args.data_dir, args.split)
    poses_dir = os.path.join(split_dir, "poses")
    
    if not os.path.exists(poses_dir):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y: {poses_dir}")
        return
    
    # Láº¥y danh sÃ¡ch video IDs
    video_files = sorted([f.replace('.npz', '') for f in os.listdir(poses_dir) 
                         if f.endswith('.npz')])
    
    if not video_files:
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file .npz nÃ o trong {poses_dir}")
        return
    
    if args.sample_idx >= len(video_files):
        print(f"âš ï¸ sample_idx={args.sample_idx} vÆ°á»£t quÃ¡ sá»‘ lÆ°á»£ng máº«u ({len(video_files)})")
        args.sample_idx = 0
    
    video_id = video_files[args.sample_idx]
    print(f"\nğŸ” Äang test máº«u: {video_id} (idx={args.sample_idx}/{len(video_files)})")
    
    # Load pose data [T, 214]
    pose_original, T = load_sample(video_id, split_dir)
    
    if pose_original is None:
        print(f"âŒ KhÃ´ng load Ä‘Æ°á»£c sample {video_id}")
        return
    
    print(f"   âœ… Loaded: shape={pose_original.shape}, length={T}")
    
    # ============================================================
    # 4. NORMALIZE (nhÆ° trong training)
    # ============================================================
    print("\nğŸ”„ Normalizing vá»›i grouped stats...")
    pose_normalized = normalize_pose(pose_original, stats)
    
    # Convert to tensor
    pose_tensor = torch.from_numpy(pose_normalized).float().unsqueeze(0).to(device)  # [1, T, 214]
    
    # Create mask (all valid)
    mask = torch.ones(1, T, dtype=torch.bool, device=device)  # [1, T]
    
    # ============================================================
    # 5. INFERENCE - RECONSTRUCTION
    # ============================================================
    print("ğŸ¤– Äang cháº¡y Autoencoder...")
    with torch.no_grad():
        reconstructed, latent = model(pose_tensor, mask=mask)
    
    print(f"   âœ… Reconstructed shape: {reconstructed.shape}")
    print(f"   âœ… Latent shape: {latent.shape}")
    
    # ============================================================
    # 6. DENORMALIZE
    # ============================================================
    print("\nğŸ”„ Denormalizing vá» tá»a Ä‘á»™ gá»‘c...")
    reconstructed_np = reconstructed.squeeze(0).cpu().numpy()  # [T, 214]
    
    # Denormalize vá»›i grouped stats
    reconstructed_denorm = denormalize_pose(reconstructed_np, stats)
    
    # ============================================================
    # 7. COMPUTE METRICS
    # ============================================================
    print("\nğŸ“Š Computing metrics...")
    
    # MSE per feature
    mse_per_feature = np.mean((pose_original - reconstructed_denorm) ** 2, axis=0)  # [214]
    
    # Split metrics
    manual_mse = np.mean(mse_per_feature[:150])  # Manual (150 dims)
    nmm_mse = np.mean(mse_per_feature[150:])     # NMM (64 dims)
    total_mse = np.mean(mse_per_feature)
    
    print(f"   ğŸ“Œ Total MSE: {total_mse:.6f}")
    print(f"   ğŸ“Œ Manual MSE (pose): {manual_mse:.6f}")
    print(f"   ğŸ“Œ NMM MSE (facial): {nmm_mse:.6f}")
    
    # MAE
    mae_per_feature = np.mean(np.abs(pose_original - reconstructed_denorm), axis=0)
    manual_mae = np.mean(mae_per_feature[:150])
    nmm_mae = np.mean(mae_per_feature[150:])
    total_mae = np.mean(mae_per_feature)
    
    print(f"   ğŸ“Œ Total MAE: {total_mae:.6f}")
    print(f"   ğŸ“Œ Manual MAE: {manual_mae:.6f}")
    print(f"   ğŸ“Œ NMM MAE: {nmm_mae:.6f}")
    
    # ============================================================
    # 8. SAVE RESULTS
    # ============================================================
    print("\nğŸ’¾ Äang lÆ°u káº¿t quáº£...")
    
    # Save original
    original_path = os.path.join(args.output_dir, f"{video_id}_original.npy")
    np.save(original_path, pose_original)
    print(f"   âœ… Saved: {original_path}")
    
    # Save reconstructed
    recon_path = os.path.join(args.output_dir, f"{video_id}_reconstructed.npy")
    np.save(recon_path, reconstructed_denorm)
    print(f"   âœ… Saved: {recon_path}")
    
    # Save latent
    latent_path = os.path.join(args.output_dir, f"{video_id}_latent.npy")
    np.save(latent_path, latent.squeeze(0).cpu().numpy())
    print(f"   âœ… Saved: {latent_path}")
    
    # Save metrics
    metrics_path = os.path.join(args.output_dir, f"{video_id}_metrics.txt")
    with open(metrics_path, 'w') as f:
        f.write(f"Video ID: {video_id}\n")
        f.write(f"Sequence Length: {T}\n")
        f.write(f"\n=== MSE ===\n")
        f.write(f"Total MSE: {total_mse:.6f}\n")
        f.write(f"Manual MSE: {manual_mse:.6f}\n")
        f.write(f"NMM MSE: {nmm_mse:.6f}\n")
        f.write(f"\n=== MAE ===\n")
        f.write(f"Total MAE: {total_mae:.6f}\n")
        f.write(f"Manual MAE: {manual_mae:.6f}\n")
        f.write(f"NMM MAE: {nmm_mae:.6f}\n")
        f.write(f"\n=== Per-Feature Stats ===\n")
        f.write(f"Manual features (0-149):\n")
        f.write(f"  MSE range: [{mse_per_feature[:150].min():.6f}, {mse_per_feature[:150].max():.6f}]\n")
        f.write(f"  MAE range: [{mae_per_feature[:150].min():.6f}, {mae_per_feature[:150].max():.6f}]\n")
        f.write(f"\nNMM features (150-213):\n")
        f.write(f"  MSE range: [{mse_per_feature[150:].min():.6f}, {mse_per_feature[150:].max():.6f}]\n")
        f.write(f"  MAE range: [{mae_per_feature[150:].min():.6f}, {mae_per_feature[150:].max():.6f}]\n")
    
    print(f"   âœ… Saved: {metrics_path}")
    
    # ============================================================
    # 9. SUMMARY
    # ============================================================
    print("\n" + "="*60)
    print("âœ… HOÃ€N Táº¤T!")
    print("="*60)
    print(f"ğŸ“‚ Káº¿t quáº£ lÆ°u táº¡i: {args.output_dir}/")
    print(f"   â€¢ Original:      {video_id}_original.npy")
    print(f"   â€¢ Reconstructed: {video_id}_reconstructed.npy")
    print(f"   â€¢ Latent:        {video_id}_latent.npy")
    print(f"   â€¢ Metrics:       {video_id}_metrics.txt")
    print("="*60)
    
    # ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng
    print("\nğŸ“‹ ÄÃNH GIÃ CHáº¤T LÆ¯á»¢NG:")
    if total_mse < 0.01:
        print("   âœ… Xuáº¥t sáº¯c! MSE < 0.01")
    elif total_mse < 0.05:
        print("   âœ… Tá»‘t! MSE < 0.05")
    elif total_mse < 0.1:
        print("   âš ï¸  Cháº¥p nháº­n Ä‘Æ°á»£c. MSE < 0.1")
    else:
        print("   âŒ KÃ©m! MSE > 0.1 - Cáº§n train thÃªm")
    
    print("\nğŸ’¡ HÆ¯á»šNG DáºªN TIáº¾P THEO:")
    print("1. Xem metrics chi tiáº¿t:")
    print(f"   cat {metrics_path}")
    print("\n2. Visualize so sÃ¡nh (náº¿u cÃ³ script):")
    print(f"   python visualize_comparison.py \\")
    print(f"     --original {original_path} \\")
    print(f"     --reconstructed {recon_path}")
    print("\n3. Test vá»›i nhiá»u máº«u khÃ¡c:")
    print(f"   python check_stage1_autoencoder.py \\")
    print(f"     --data_dir {args.data_dir} \\")
    print(f"     --autoencoder_checkpoint {args.autoencoder_checkpoint} \\")
    print(f"     --sample_idx [0-{len(video_files)-1}]")


if __name__ == '__main__':
    main()