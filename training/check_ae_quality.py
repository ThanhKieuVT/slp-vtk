import torch
import numpy as np
import argparse
import os
import sys
import glob

# --- Import Modules (Xá»­ lÃ½ Ä‘Æ°á»ng dáº«n Colab/Local) ---
try:
    sys.path.append(os.getcwd()) 
    # Import Model cá»§a Stage 1
    from models.fml.autoencoder import UnifiedPoseAutoencoder
    # Import logic chuáº©n tá»« file chá»‹ gá»­i
    from data_preparation import normalize_pose, denormalize_pose 
except ImportError as e:
    print(f"âŒ Lá»—i Import: {e}")
    print("ğŸ’¡ Gá»£i Ã½: Chá»‹ nhá»› Ä‘áº·t file nÃ y á»Ÿ thÆ° má»¥c gá»‘c (cÃ¹ng cáº¥p vá»›i folder 'models' vÃ  'data_preparation.py')")
    sys.exit(1)

def main():
    # --- 1. Cáº¤U HÃŒNH THAM Sá» ---
    parser = argparse.ArgumentParser(description='Kiá»ƒm tra cháº¥t lÆ°á»£ng Autoencoder (Stage 1)')
    
    parser.add_argument('--data_dir', type=str, required=True,
                        help='Folder chá»©a file normalization_stats.npz vÃ  cÃ¡c folder train/dev/test')
    parser.add_argument('--autoencoder_checkpoint', type=str, required=True,
                        help='ÄÆ°á»ng dáº«n file .pt cá»§a Autoencoder')
    parser.add_argument('--output_dir', type=str, default='check_stage1_output',
                        help='NÆ¡i lÆ°u file káº¿t quáº£')
    
    # Config Model (Pháº£i khá»›p lÃºc chá»‹ train Stage 1)
    parser.add_argument('--latent_dim', type=int, default=256)
    parser.add_argument('--hidden_dim', type=int, default=512)

    args = parser.parse_args()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ Äang kiá»ƒm tra trÃªn device: {device}")
    os.makedirs(args.output_dir, exist_ok=True)

    # --- 2. LOAD STATS (Theo cáº¥u trÃºc Grouped Stats cá»§a chá»‹) ---
    stats_path = os.path.join(args.data_dir, "normalization_stats.npz")
    if not os.path.exists(stats_path):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y normalization_stats.npz táº¡i {args.data_dir}")
        return
        
    print(f"ğŸ“Š Loading Grouped Stats tá»«: {stats_path}")
    stats_npz = np.load(stats_path)
    # Chuyá»ƒn vá» dictionary Ä‘á»ƒ truyá»n vÃ o hÃ m normalize_pose
    stats = {k: stats_npz[k] for k in stats_npz.files}

    # --- 3. LOAD MODEL ---
    print(f"ğŸ“¦ Loading Autoencoder...")
    ae = UnifiedPoseAutoencoder(
        pose_dim=214, 
        latent_dim=args.latent_dim, 
        hidden_dim=args.hidden_dim
    ).to(device)

    try:
        ckpt = torch.load(args.autoencoder_checkpoint, map_location=device)
        state_dict = ckpt['model_state_dict'] if 'model_state_dict' in ckpt else ckpt
        ae.load_state_dict(state_dict, strict=True)
        print("âœ… Load weights thÃ nh cÃ´ng!")
        ae.eval()
    except Exception as e:
        print(f"âŒ Lá»—i load checkpoint: {e}")
        return

    # --- 4. TÃŒM & LOAD FILE DATA MáºªU ---
    # Tá»± Ä‘á»™ng tÃ¬m 1 file .npz (theo format load_sample cá»§a chá»‹) hoáº·c .npy
    print(f"ğŸ” Äang tÃ¬m file máº«u...")
    files = glob.glob(os.path.join(args.data_dir, "**/*.npz"), recursive=True) + \
            glob.glob(os.path.join(args.data_dir, "**/*.npy"), recursive=True)
    
    valid_files = [f for f in files if "stats" not in f and "output" not in f]
    
    if not valid_files:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u nÃ o Ä‘á»ƒ test!")
        return

    sample_file = valid_files[0]
    print(f"âœ… ÄÃ£ chá»n file máº«u: {sample_file}")
    
    # á» Stage 1, Model nháº­n Ä‘áº§u vÃ o 214D. 
    # Náº¿u file chá»‹ chá»n lÃ  file raw, chá»‹ nÃªn dÃ¹ng hÃ m load_sample cá»§a chá»‹ Ä‘á»ƒ cÃ³ Ä‘Ãºng 214D
    # á» Ä‘Ã¢y em giáº£ Ä‘á»‹nh chá»‹ trá» vÃ o file Ä‘Ã£ xá»­ lÃ½ hoáº·c em sáº½ láº¥y key keypoints náº¿u cÃ³.
    data = np.load(sample_file)
    if isinstance(data, np.lib.npyio.NpzFile):
        # Æ¯u tiÃªn láº¥y keypoints náº¿u lÃ  file pose 
        real_pose_np = data['keypoints'] if 'keypoints' in data else data[data.files[0]]
    else:
        real_pose_np = data

    # Flatten náº¿u cáº§n (pháº£i Ä‘Æ°a vá» [T, 214])
    if len(real_pose_np.shape) == 3: # [T, 75, 2] -> [T, 150]
        real_pose_np = real_pose_np.reshape(real_pose_np.shape[0], -1)
        print(f"âš ï¸ Cáº£nh bÃ¡o: Dá»¯ liá»‡u tá»± Ä‘á»™ng flatten vá» {real_pose_np.shape}. HÃ£y cháº¯c cháº¯n nÃ³ lÃ  214D.")

    # --- 5. NORMALIZE & INFERENCE ---
    # Sá»­ dá»¥ng hÃ m cá»§a chá»‹ Ä‘á»ƒ Ä‘áº£m báº£o Ä‘Ãºng logic Grouped Normalization
    real_pose_norm = normalize_pose(real_pose_np, stats)
    
    # Chuyá»ƒn sang Tensor vÃ  thÃªm Batch Dimension [1, T, 214]
    real_pose_input = torch.tensor(real_pose_norm, dtype=torch.float32).to(device).unsqueeze(0)

    print("ğŸ”„ Äang cháº¡y qua Autoencoder...")
    with torch.no_grad():
        recon_norm, _ = ae(real_pose_input)

    # --- 6. DENORMALIZE & SAVE ---
    recon_norm_np = recon_norm.squeeze(0).cpu().numpy()
    
    # Giáº£i chuáº©n hÃ³a vá» scale gá»‘c báº±ng chÃ­nh hÃ m cá»§a chá»‹
    recon_final = denormalize_pose(recon_norm_np, stats)
    
    # TÃ­nh lá»—i MSE cÆ¡ báº£n Ä‘á»ƒ chá»‹ Ä‘Ã¡nh giÃ¡ nhanh qua terminal
    mse = np.mean((real_pose_np - recon_final)**2)
    print(f"ğŸ“‰ Reconstruction MSE (Original Scale): {mse:.6f}")

    # LÆ°u káº¿t quáº£ Ä‘á»ƒ so sÃ¡nh
    real_save_path = os.path.join(args.output_dir, "original_sample.npy")
    recon_save_path = os.path.join(args.output_dir, "reconstructed_sample.npy")
    
    np.save(real_save_path, real_pose_np)
    np.save(recon_save_path, recon_final)
    
    print("\n" + "="*40)
    print("ğŸ‰ HOÃ€N Táº¤T!")
    print(f"   1. Gá»‘c:      {real_save_path}")
    print(f"   2. TÃ¡i táº¡o:  {recon_save_path}")
    print("="*40)
    print(f"\nğŸ‘‰ Chá»‹ cháº¡y lá»‡nh sau Ä‘á»ƒ xem video so sÃ¡nh:")
    print(f"python training/visualize_single_pose.py --npy_path {recon_save_path}")

if __name__ == '__main__':
    main()