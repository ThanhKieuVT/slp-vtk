# TÃªn file: check_autoencoder.py
import os
import argparse
import torch
import numpy as np
from torch.utils.data import DataLoader

# Import tá»« cÃ¡c file cá»§a báº¡n
from dataset import SignLanguageDataset, collate_fn
from models.fml.autoencoder import UnifiedPoseAutoencoder
from data_preparation import denormalize_pose

def check_reconstruction(args):
    """
    Táº£i mÃ´ hÃ¬nh Stage 1, cháº¡y tÃ¡i táº¡o trÃªn 1 sample,
    giáº£i chuáº©n hÃ³a vÃ  lÆ°u káº¿t quáº£.
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Sá»­ dá»¥ng thiáº¿t bá»‹: {device}")

    # --- 1. Táº£i Normalization Stats ---
    stats_path = os.path.join(args.data_dir, "normalization_stats.npz")
    if not os.path.exists(stats_path):
        print(f"Lá»—i: KhÃ´ng tÃ¬m tháº¥y {stats_path}")
        return
    
    stats = np.load(stats_path)
    mean = stats['mean']
    std = stats['std']
    print(f"âœ… ÄÃ£ táº£i stats tá»« {stats_path}")

    # --- 2. Táº£i Autoencoder (Stage 1) ---
    print(f"ğŸ“¦ Äang táº£i autoencoder tá»« {args.autoencoder_checkpoint}")
    autoencoder = UnifiedPoseAutoencoder(
        pose_dim=214,
        latent_dim=args.latent_dim,
        hidden_dim=args.hidden_dim
    )
    checkpoint = torch.load(args.autoencoder_checkpoint, map_location=device)
    autoencoder.load_state_dict(checkpoint['model_state_dict'])
    autoencoder.to(device)
    autoencoder.eval()
    print("âœ… Autoencoder Ä‘Ã£ Ä‘Æ°á»£c táº£i")

    # --- 3. Táº£i Dataset (dÃ¹ng táº­p 'dev' Ä‘á»ƒ kiá»ƒm tra) ---
    print(f"ğŸ“‚ Äang táº£i {args.split} dataset...")
    dataset = SignLanguageDataset(
        data_dir=args.data_dir,
        split=args.split,
        max_seq_len=args.max_seq_len,
        stats_path=stats_path
    )
    
    if args.sample_idx >= len(dataset):
        print(f"Lá»—i: sample_idx {args.sample_idx} vÆ°á»£t quÃ¡ sá»‘ lÆ°á»£ng máº«u {len(dataset)}")
        return

    # Láº¥y 1 sample
    sample = dataset[args.sample_idx]
    
    # Táº¡o batch (batch_size=1)
    batch = collate_fn([sample])
    
    # Chuyá»ƒn batch lÃªn device
    poses_gt_norm = batch['poses'].to(device) # ÄÃ¢y lÃ  pose Ä‘Ã£ chuáº©n hÃ³a
    pose_mask = batch['pose_mask'].to(device)
    seq_length = batch['seq_lengths'][0].item()
    video_id = batch['video_ids'][0]
    
    print(f"âœ… ÄÃ£ táº£i sample: {video_id} (index {args.sample_idx}), Ä‘á»™ dÃ i: {seq_length} frames")

    # --- 4. Cháº¡y tÃ¡i táº¡o ---
    with torch.no_grad():
        reconstructed_pose_norm, _ = autoencoder(poses_gt_norm, mask=pose_mask)
    
    # Chuyá»ƒn vá» numpy
    poses_gt_norm_np = poses_gt_norm.squeeze(0).cpu().numpy()
    reconstructed_pose_norm_np = reconstructed_pose_norm.squeeze(0).cpu().numpy()
    
    # Cáº¯t bá» padding
    poses_gt_norm_np = poses_gt_norm_np[:seq_length]
    reconstructed_pose_norm_np = reconstructed_pose_norm_np[:seq_length]

    # --- 5. Giáº£i chuáº©n hÃ³a (QUAN TRá»ŒNG) ---
    pose_gt_denorm = denormalize_pose(poses_gt_norm_np, mean, std)
    pose_recon_denorm = denormalize_pose(reconstructed_pose_norm_np, mean, std)
    print("âœ… ÄÃ£ giáº£i chuáº©n hÃ³a (denormalize) 2 poses")

    # --- 6. LÆ°u káº¿t quáº£ ---
    os.makedirs(args.output_dir, exist_ok=True)
    gt_path = os.path.join(args.output_dir, f"{video_id}_gt.npy")
    recon_path = os.path.join(args.output_dir, f"{video_id}_recon.npy")
    
    np.save(gt_path, pose_gt_denorm)
    np.save(recon_path, pose_recon_denorm)
    
    print(f"\nğŸ‰ ThÃ nh cÃ´ng!")
    print(f"  ÄÃ£ lÆ°u Ground Truth: {gt_path}")
    print(f"  ÄÃ£ lÆ°u Reconstructed: {recon_path}")
    print(f"\nğŸ‘‰ BÆ°á»›c tiáº¿p theo: Cháº¡y visualize_pose.py Ä‘á»ƒ xem káº¿t quáº£:")
    print(f"python visualize_pose.py --gt_path {gt_path} --recon_path {recon_path}")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Kiá»ƒm tra cháº¥t lÆ°á»£ng Autoencoder (Stage 1)')
    
    parser.add_argument('--data_dir', type=str, required=True,
                        help='ÄÆ°á»ng dáº«n Ä‘áº¿n processed_data/data/')
    parser.add_argument('--autoencoder_checkpoint', type=str, required=True,
                        help='Checkpoint cá»§a Autoencoder (best_model.pt cá»§a Stage 1)')
    parser.add_argument('--output_dir', type=str, default='check_stage1_output',
                        help='ThÆ° má»¥c lÆ°u 2 file .npy')
    
    # CÃ¡c tham sá»‘ nÃ y pháº£i khá»›p vá»›i lÃºc báº¡n train Stage 1
    parser.add_argument('--latent_dim', type=int, default=256, help='Latent dimension')
    parser.add_argument('--hidden_dim', type=int, default=512, help='Hidden dimension')
    parser.add_argument('--max_seq_len', type=int, default=120, help='Max sequence length')
    
    parser.add_argument('--split', type=str, default='dev', choices=['train', 'dev', 'test'],
                        help='Dataset split Ä‘á»ƒ láº¥y máº«u')
    parser.add_argument('--sample_idx', type=int, default=0,
                        help='Index cá»§a sample trong dataset')
    
    args = parser.parse_args()
    
    check_reconstruction(args)