# =============================================================================
# âœ… FIXED: data_preparation.py
# =============================================================================
"""
Data Preparation vá»›i grouped normalization
- Manual features: scalar mean/std
- NMM features: per-feature mean/std
"""
import os
import pickle
import numpy as np
from tqdm import tqdm


def load_sample(video_id, split_dir):
    """
    Load pose + NMM data vÃ  combine thÃ nh 214D vector
    
    Returns:
        pose_214: [T, 214] numpy array
        T: Sequence length
    """
    pose_path = os.path.join(split_dir, "poses", f"{video_id}.npz")
    nmm_path = os.path.join(split_dir, "nmms", f"{video_id}.pkl")
    
    if not os.path.exists(pose_path) or not os.path.exists(nmm_path):
        return None, 0
    
    try:
        # Load poses
        poses = np.load(pose_path)
        kp = poses["keypoints"]  # [T, 75, 2]
        
        # Load NMMs
        with open(nmm_path, 'rb') as f:
            nmms = pickle.load(f)
        
        # Ensure length match
        T = min(len(kp), len(nmms["facial_aus"]))
        if T == 0:
            return None, 0
        
        # Combine manual: [T, 75, 2] -> [T, 150]
        manual = kp[:T].reshape(T, -1)
        
        # Combine NMMs: 17 + 3 + 4 + 40 = 64
        aus = nmms["facial_aus"][:T]
        head = nmms["head_pose"][:T]
        gaze = nmms["eye_gaze"][:T]
        mouth = nmms["mouth_shape"][:T].reshape(T, -1)
        nmm = np.concatenate([aus, head, gaze, mouth], axis=-1)
        
        # Final: [T, 214] = 150 (manual) + 64 (NMM)
        pose_214 = np.concatenate([manual, nmm], axis=-1)
        return pose_214, T
        
    except Exception as e:
        print(f"Error loading {video_id}: {e}")
        return None, 0


def compute_normalization_stats(data_dir, split='train'):
    """
    âœ… FIXED: Grouped normalization
    
    Returns:
        dict with keys: 'manual_mean', 'manual_std', 'nmm_mean', 'nmm_std'
    """
    split_dir = os.path.join(data_dir, split)
    poses_dir = os.path.join(split_dir, "poses")
    
    if not os.path.exists(poses_dir):
        raise ValueError(f"Not found: {poses_dir}")
    
    video_ids = [f.replace('.npz', '') for f in os.listdir(poses_dir) 
                 if f.endswith('.npz')]
    
    print(f"Computing normalization stats from {len(video_ids)} videos...")
    
    manual_values = []
    nmm_values = []
    
    for video_id in tqdm(video_ids):
        pose_214, T = load_sample(video_id, split_dir)
        if pose_214 is not None and T > 0:
            manual_values.append(pose_214[:, :150].flatten())
            nmm_values.append(pose_214[:, 150:])
    
    if not manual_values:
        raise ValueError("No valid data!")
    
    # Manual: scalar stats
    all_manual = np.concatenate(manual_values)
    manual_mean = float(np.mean(all_manual))
    manual_std = float(np.std(all_manual))
    
    # NMM: per-feature stats
    all_nmm = np.concatenate(nmm_values, axis=0)
    nmm_mean = np.mean(all_nmm, axis=0)  # [64]
    nmm_std = np.std(all_nmm, axis=0)    # [64]
    nmm_std = np.where(nmm_std < 1e-6, 1.0, nmm_std)
    
    print(f"\nâœ… Stats computed:")
    print(f"   Manual: mean={manual_mean:.4f}, std={manual_std:.4f}")
    print(f"   NMM: mean shape={nmm_mean.shape}, std shape={nmm_std.shape}")
    
    return {
        'manual_mean': manual_mean,
        'manual_std': manual_std,
        'nmm_mean': nmm_mean,
        'nmm_std': nmm_std
    }


def normalize_pose(pose_214, stats):
    """
    âœ… FIXED: Apply grouped normalization
    
    Args:
        pose_214: [T, 214] numpy array
        stats: dict with 'manual_mean', 'manual_std', 'nmm_mean', 'nmm_std'
    """
    if stats is None:
        return pose_214
    
    pose_norm = pose_214.copy()
    
    # Normalize manual part (0:150)
    pose_norm[..., :150] = (pose_norm[..., :150] - stats['manual_mean']) / \
                           (stats['manual_std'] + 1e-8)
    
    # Normalize NMM part (150:214)
    pose_norm[..., 150:] = (pose_norm[..., 150:] - stats['nmm_mean']) / \
                           (stats['nmm_std'] + 1e-8)
    
    return pose_norm


def denormalize_pose(normalized_pose, stats):
    """Denormalize pose back to original scale"""
    if stats is None:
        return normalized_pose
    
    pose_denorm = normalized_pose.copy()
    
    pose_denorm[..., :150] = pose_denorm[..., :150] * stats['manual_std'] + \
                             stats['manual_mean']
    
    pose_denorm[..., 150:] = pose_denorm[..., 150:] * stats['nmm_std'] + \
                             stats['nmm_mean']
    
    return pose_denorm


if __name__ == "__main__":
    DATA_DIR = "path/to/processed_data/data"
    
    # Compute and save stats
    stats = compute_normalization_stats(DATA_DIR, split='train')
    
    stats_path = os.path.join(DATA_DIR, "normalization_stats.npz")
    np.savez_compressed(stats_path, **stats)
    print(f"âœ… Saved stats to: {stats_path}")


# =============================================================================
# âœ… FIXED: dataset.py
# =============================================================================
"""
PyTorch Dataset with:
- âœ… Grouped normalization support
- âœ… Text loading from corpus.csv
- âœ… Dynamic padding in collate_fn
"""
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer
import pandas as pd


class SignLanguageDataset(Dataset):
    def __init__(
        self,
        data_dir,
        split=None,
        phoenix_root=None,
        max_seq_len=400,
        max_text_len=128,
        normalize=True,
        stats_path=None
    ):
        self.data_dir = data_dir
        self.max_seq_len = max_seq_len
        self.max_text_len = max_text_len
        self.normalize = normalize
        
        # Auto-detect split
        if split is None:
            basename = os.path.basename(data_dir.rstrip('/'))
            if basename in ['train', 'dev', 'test']:
                self.split = basename
                self.split_dir = data_dir
                self.root_dir = os.path.dirname(data_dir)
            else:
                self.split = 'train'
                self.split_dir = os.path.join(data_dir, self.split)
                self.root_dir = data_dir
        else:
            self.split = split
            self.split_dir = os.path.join(data_dir, split)
            self.root_dir = data_dir
        
        print(f"ðŸ“‚ Dataset: split={self.split}")
        
        # âœ… Load grouped stats
        if normalize:
            if stats_path is None:
                stats_path = os.path.join(self.root_dir, "normalization_stats.npz")
            
            if os.path.exists(stats_path):
                stats_data = np.load(stats_path)
                self.stats = {
                    'manual_mean': float(stats_data['manual_mean']),
                    'manual_std': float(stats_data['manual_std']),
                    'nmm_mean': stats_data['nmm_mean'],
                    'nmm_std': stats_data['nmm_std']
                }
                print(f"   âœ… Loaded grouped normalization stats")
            else:
                print(f"   âš ï¸ Stats not found, disabling normalization")
                self.normalize = False
        
        # Tokenizer
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
        
        # âœ… FIXED: Load text from corpus.csv
        poses_dir = os.path.join(self.split_dir, "poses")
        available_poses = set([f.replace('.npz', '') for f in os.listdir(poses_dir) 
                              if f.endswith('.npz')])
        
        self.video_ids = []
        self.texts = {}
        
        # Auto-detect Phoenix root
        if phoenix_root is None:
            current = self.root_dir
            while current and current != '/':
                if os.path.exists(os.path.join(current, 'PHOENIX-2014-T')):
                    phoenix_root = os.path.join(current, 'PHOENIX-2014-T')
                    break
                current = os.path.dirname(current)
        
        # Load from corpus.csv
        if phoenix_root and os.path.exists(phoenix_root):
            corpus_file = os.path.join(
                phoenix_root, 
                "annotations", "manual", 
                f"PHOENIX-2014-T.{self.split}.corpus.csv"
            )
            
            if os.path.exists(corpus_file):
                df = pd.read_csv(corpus_file, sep='|')
                for _, row in df.iterrows():
                    vid_id = row['name']
                    if vid_id in available_poses:
                        self.texts[vid_id] = row['translation']
                        self.video_ids.append(vid_id)
                print(f"   âœ… Loaded {len(self.video_ids)} samples from corpus.csv")
            else:
                print(f"   âš ï¸ Corpus file not found, using poses without text")
                self.video_ids = sorted(list(available_poses))
        else:
            print(f"   âš ï¸ Phoenix root not found, using poses without text")
            self.video_ids = sorted(list(available_poses))
    
    def __len__(self):
        return len(self.video_ids)
    
    def __getitem__(self, idx):
        video_id = self.video_ids[idx]
        
        # Load pose
        pose, T = load_sample(video_id, self.split_dir)
        if pose is None:
            return None
        
        # âœ… Normalize with grouped stats
        if self.normalize:
            pose = normalize_pose(pose, self.stats)
        
        # Crop if too long
        if T > self.max_seq_len:
            pose = pose[:self.max_seq_len]
            T = self.max_seq_len
        
        pose_tensor = torch.FloatTensor(pose)
        
        # Get text
        text = self.texts.get(video_id, "")
        encoded = self.tokenizer(
            text,
            max_length=self.max_text_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'video_id': video_id,
            'pose': pose_tensor,
            'seq_length': T,
            'text': text,
            'text_tokens': encoded['input_ids'].squeeze(0),
            'attention_mask': encoded['attention_mask'].squeeze(0),
        }


def collate_fn(batch):
    """Collate with dynamic padding"""
    batch = [item for item in batch if item is not None]
    if not batch:
        return None
    
    video_ids = [item['video_id'] for item in batch]
    texts = [item['text'] for item in batch]
    seq_lengths = torch.LongTensor([item['seq_length'] for item in batch])
    text_tokens = torch.stack([item['text_tokens'] for item in batch])
    attention_masks = torch.stack([item['attention_mask'] for item in batch])
    
    # Dynamic padding
    poses_list = [item['pose'] for item in batch]
    poses_padded = torch.nn.utils.rnn.pad_sequence(
        poses_list, batch_first=True, padding_value=0.0
    )
    
    # Mask: True=valid, False=padding
    max_len = poses_padded.shape[1]
    pose_mask = torch.arange(max_len)[None, :] < seq_lengths[:, None]
    
    return {
        'video_ids': video_ids,
        'poses': poses_padded,
        'pose_mask': pose_mask,
        'seq_lengths': seq_lengths,
        'text_tokens': text_tokens,
        'attention_mask': attention_masks,
        'texts': texts
    }


# =============================================================================
# âœ… NEW: flow_matching.py (Missing Module)
# =============================================================================
"""Flow Matching components"""
import torch
import torch.nn as nn
import math


class FlowMatchingScheduler:
    """Flow matching time scheduler"""
    
    def sample_timesteps(self, batch_size, device):
        """Sample random timesteps uniformly"""
        return torch.rand(batch_size, device=device)
    
    def add_noise(self, x0, t):
        """
        Add noise using optimal transport flow
        
        Args:
            x0: [B, T, D] clean data
            t: [B] timesteps in [0, 1]
        
        Returns:
            xt: [B, T, D] noisy data
            v_gt: [B, T, D] ground truth velocity
            noise: [B, T, D] noise vector
        """
        noise = torch.randn_like(x0)
        t = t.view(-1, 1, 1)  # [B, 1, 1]
        
        # Linear interpolation (Optimal Transport path)
        xt = t * x0 + (1 - t) * noise
        
        # Ground truth velocity
        v_gt = x0 - noise
        
        return xt, v_gt, noise


class FlowMatchingLoss(nn.Module):
    """Flow matching loss"""
    
    def forward(self, v_pred, v_target, mask=None):
        """
        Args:
            v_pred: [B, T, D] predicted velocity
            v_target: [B, T, D] target velocity
            mask: [B, T] valid positions (True=valid)
        """
        loss = (v_pred - v_target) ** 2
        
        if mask is not None:
            loss = loss * mask.unsqueeze(-1).float()
            return loss.sum() / (mask.sum() * loss.shape[-1]).clamp(min=1)
        else:
            return loss.mean()


class FlowMatchingBlock(nn.Module):
    """Transformer-based flow matching block"""
    
    def __init__(
        self,
        data_dim,
        condition_dim,
        hidden_dim,
        num_layers=6,
        num_heads=8,
        dropout=0.1
    ):
        super().__init__()
        
        self.data_dim = data_dim
        self.hidden_dim = hidden_dim
        
        # Time embedding
        self.time_mlp = nn.Sequential(
            nn.Linear(1, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # Input projection
        self.data_proj = nn.Linear(data_dim, hidden_dim)
        
        # Transformer layers
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim * 4,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # Output projection
        self.output_proj = nn.Linear(hidden_dim, data_dim)
    
    def forward(
        self, 
        x, 
        t, 
        condition, 
        text_attn_mask=None, 
        pose_attn_mask=None,
        return_attn=False
    ):
        """
        Args:
            x: [B, T, data_dim] noisy data
            t: [B] timesteps
            condition: [B, L, condition_dim] text features
            text_attn_mask: [B, L] True=padding (inverted mask)
            pose_attn_mask: [B, T] True=padding (inverted mask)
        
        Returns:
            v_pred: [B, T, data_dim] predicted velocity
        """
        B, T, _ = x.shape
        
        # Time embedding
        t_emb = self.time_mlp(t.unsqueeze(-1))  # [B, hidden_dim]
        t_emb = t_emb.unsqueeze(1).expand(-1, T, -1)  # [B, T, hidden_dim]
        
        # Project data
        x_emb = self.data_proj(x)  # [B, T, hidden_dim]
        
        # Add time embedding
        x_emb = x_emb + t_emb
        
        # Concatenate with condition
        full_seq = torch.cat([condition, x_emb], dim=1)  # [B, L+T, hidden_dim]
        
        # Create attention mask
        if text_attn_mask is not None and pose_attn_mask is not None:
            full_mask = torch.cat([text_attn_mask, pose_attn_mask], dim=1)
        else:
            full_mask = None
        
        # Transform
        output = self.transformer(full_seq, src_key_padding_mask=full_mask)
        
        # Extract pose part
        pose_output = output[:, -T:, :]  # [B, T, hidden_dim]
        
        # Predict velocity
        v_pred = self.output_proj(pose_output)  # [B, T, data_dim]
        
        if return_attn:
            return v_pred, None
        return v_pred


# =============================================================================
# âœ… NEW: mamba_prior.py (Missing Module)
# =============================================================================
"""Simplified SSM Prior (using GRU as placeholder)"""
import torch.nn as nn


class SimpleSSMPrior(nn.Module):
    """
    Simplified State Space Model Prior
    Uses GRU as a simple implementation
    """
    
    def __init__(
        self,
        latent_dim,
        hidden_dim,
        num_layers=4,
        num_heads=8,
        dropout=0.1
    ):
        super().__init__()
        
        self.latent_proj = nn.Linear(latent_dim, hidden_dim)
        
        self.gru = nn.GRU(
            hidden_dim,
            hidden_dim,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True
        )
        
        self.output_proj = nn.Linear(hidden_dim, latent_dim)
    
    def forward(self, latent, t, condition, mask=None):
        """
        Args:
            latent: [B, T, latent_dim]
            t: [B] timesteps (unused in simple version)
            condition: [B, L, condition_dim] (unused in simple version)
            mask: [B, T] True=valid
        
        Returns:
            v_prior: [B, T, latent_dim]
        """
        # Project
        x = self.latent_proj(latent)  # [B, T, hidden_dim]
        
        # GRU forward
        output, _ = self.gru(x)  # [B, T, hidden_dim]
        
        # Project back
        v_prior = self.output_proj(output)  # [B, T, latent_dim]
        
        return v_prior


# =============================================================================
# âœ… NEW: sync_guidance.py (Missing Module)
# =============================================================================
"""Synchronization guidance head"""
import torch.nn as nn


class SyncGuidanceHead(nn.Module):
    """
    Compute synchronization score between latent and text
    """
    
    def __init__(self, latent_dim, hidden_dim, dropout=0.1):
        super().__init__()
        
        self.latent_proj = nn.Linear(latent_dim, hidden_dim)
        self.text_proj = nn.Linear(latent_dim, hidden_dim)
        
        self.score_net = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, latent, text_pooled, mask=None):
        """
        Args:
            latent: [B, T, latent_dim]
            text_pooled: [B, latent_dim]
            mask: [B, T] True=valid
        
        Returns:
            score: [B, T] synchronization scores
        """
        B, T, _ = latent.shape
        
        # Project
        latent_emb = self.latent_proj(latent)  # [B, T, hidden_dim]
        text_emb = self.text_proj(text_pooled).unsqueeze(1).expand(-1, T, -1)  # [B, T, hidden_dim]
        
        # Concatenate
        combined = torch.cat([latent_emb, text_emb], dim=-1)  # [B, T, hidden_dim*2]
        
        # Score
        score = self.score_net(combined).squeeze(-1)  # [B, T]
        
        return score


# =============================================================================
# USAGE EXAMPLE
# =============================================================================
if __name__ == "__main__":
    print("="*60)
    print("TESTING FIXED CODE")
    print("="*60)
    
    # Step 1: Compute stats
    DATA_DIR = "/Users/kieuvo/Learn/Research/SL/Implement/SignFML/slp-vtk/data/RWTH/PHOENIX-2014-T-release-v3/processed_data"
    PHOENIX_ROOT = "/Users/kieuvo/Learn/Research/SL/Implement/SignFML/slp-vtk/data/RWTH/PHOENIX-2014-T-release-v3/PHOENIX-2014-T"
    
    print("\n1. Computing normalization stats...")
    stats = compute_normalization_stats(DATA_DIR, split='train')
    
    # Save stats
    stats_path = os.path.join(DATA_DIR, "normalization_stats.npz")
    np.savez_compressed(stats_path, **stats)
    print(f"   âœ… Saved to: {stats_path}")
    
    # Step 2: Create dataset
    print("\n2. Creating dataset...")
    dataset = SignLanguageDataset(
        data_dir=DATA_DIR,
        split='train',
        phoenix_root=PHOENIX_ROOT,
        max_seq_len=400
    )
    
    print(f"   âœ… Dataset size: {len(dataset)}")
    
    # Step 3: Test dataloader
    print("\n3. Testing dataloader...")
    loader = DataLoader(
        dataset,
        batch_size=4,
        shuffle=False,
        collate_fn=collate_fn
    )
    
    batch = next(iter(loader))
    if batch:
        print(f"   âœ… Batch shapes:")
        print(f"      poses: {batch['poses'].shape}")
        print(f"      pose_mask: {batch['pose_mask'].shape}")
        print(f"      text_tokens: {batch['text_tokens'].shape}")
    
    # Step 4: Test flow matching components
    print("\n4. Testing flow matching components...")
    scheduler = FlowMatchingScheduler()
    flow_block = FlowMatchingBlock(
        data_dim=256,
        condition_dim=512,
        hidden_dim=512
    )
    
    print("   âœ… All components initialized successfully!")
    
    print("\n" + "="*60)
    print("âœ… ALL TESTS PASSED - CODE IS READY TO RUN")
    print("="*60)